<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.489">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Workshop: Effective Graphics for Visual Communication with Data - Fundamentals of Graphical Communication</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Workshop: Effective Graphics for Visual Communication with Data</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> 
<span class="menu-text">Course Description</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-materials" role="button" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Materials</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-materials">    
        <li>
    <a class="dropdown-item" href="../content/contents.html">
 <span class="dropdown-text">Workshop Materials</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../content/00-Setup.html">
 <span class="dropdown-text">Setting Up for the Workshop</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../content/01-Fundamentals.html">
 <span class="dropdown-text">Fundamentals of Graphical Communication</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../content/02-eda.html">
 <span class="dropdown-text">Exploratory Data Analysis</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../content/03-accessibility.html">
 <span class="dropdown-text">Accessibility: More than just Alt-text</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../content/04-rules.html">
 <span class="dropdown-text">The Rules, and when to break them…</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../content/05-workshop.html">
 <span class="dropdown-text">Workshop: Improving Our Graphics</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#a-quick-introduction-to-perception-and-cognition" id="toc-a-quick-introduction-to-perception-and-cognition" class="nav-link active" data-scroll-target="#a-quick-introduction-to-perception-and-cognition">A Quick Introduction to Perception and Cognition</a>
  <ul class="collapse">
  <li><a href="#hardware" id="toc-hardware" class="nav-link" data-scroll-target="#hardware">Hardware</a>
  <ul class="collapse">
  <li><a href="#the-eye" id="toc-the-eye" class="nav-link" data-scroll-target="#the-eye">The Eye</a></li>
  <li><a href="#the-brain" id="toc-the-brain" class="nav-link" data-scroll-target="#the-brain">The Brain</a></li>
  </ul></li>
  <li><a href="#software" id="toc-software" class="nav-link" data-scroll-target="#software">Software</a>
  <ul class="collapse">
  <li><a href="#selective-attention" id="toc-selective-attention" class="nav-link" data-scroll-target="#selective-attention">Selective Attention</a></li>
  <li><a href="#object-perception" id="toc-object-perception" class="nav-link" data-scroll-target="#object-perception">Object Perception</a></li>
  <li><a href="#memory" id="toc-memory" class="nav-link" data-scroll-target="#memory">Memory</a></li>
  <li><a href="#information-integration" id="toc-information-integration" class="nav-link" data-scroll-target="#information-integration">Information Integration</a></li>
  <li><a href="#resource-limitations" id="toc-resource-limitations" class="nav-link" data-scroll-target="#resource-limitations">Resource Limitations</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#the-psychology-of-charts" id="toc-the-psychology-of-charts" class="nav-link" data-scroll-target="#the-psychology-of-charts">The Psychology of Charts</a>
  <ul class="collapse">
  <li><a href="#preattentive-perception" id="toc-preattentive-perception" class="nav-link" data-scroll-target="#preattentive-perception">Preattentive Perception</a></li>
  <li><a href="#conscious-perception" id="toc-conscious-perception" class="nav-link" data-scroll-target="#conscious-perception">Conscious Perception</a>
  <ul class="collapse">
  <li><a href="#task-based-processing" id="toc-task-based-processing" class="nav-link" data-scroll-target="#task-based-processing">Task Based Processing</a></li>
  <li><a href="#information-integration-1" id="toc-information-integration-1" class="nav-link" data-scroll-target="#information-integration-1">Information Integration</a></li>
  </ul></li>
  <li><a href="#simple-charts" id="toc-simple-charts" class="nav-link" data-scroll-target="#simple-charts">Simple Charts</a>
  <ul class="collapse">
  <li><a href="#color-shape-and-discriminability" id="toc-color-shape-and-discriminability" class="nav-link" data-scroll-target="#color-shape-and-discriminability">Color, Shape, and Discriminability</a></li>
  <li><a href="#other-considerations" id="toc-other-considerations" class="nav-link" data-scroll-target="#other-considerations">Other Considerations</a></li>
  </ul></li>
  <li><a href="#complex-domain-specific-charts" id="toc-complex-domain-specific-charts" class="nav-link" data-scroll-target="#complex-domain-specific-charts">Complex, Domain-Specific Charts</a></li>
  </ul></li>
  <li><a href="#strategies-for-readability" id="toc-strategies-for-readability" class="nav-link" data-scroll-target="#strategies-for-readability">Strategies for Readability</a>
  <ul class="collapse">
  <li><a href="#center-primary-comparisons" id="toc-center-primary-comparisons" class="nav-link" data-scroll-target="#center-primary-comparisons">Center Primary Comparisons</a></li>
  <li><a href="#reduce-cognitive-load" id="toc-reduce-cognitive-load" class="nav-link" data-scroll-target="#reduce-cognitive-load">Reduce Cognitive Load</a></li>
  </ul></li>
  <li><a href="#testing-multiple-versions-of-a-chart" id="toc-testing-multiple-versions-of-a-chart" class="nav-link" data-scroll-target="#testing-multiple-versions-of-a-chart">Testing Multiple Versions of a Chart</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Fundamentals of Graphical Communication</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="a-quick-introduction-to-perception-and-cognition" class="level1">
<h1>A Quick Introduction to Perception and Cognition</h1>
<p>In order to design graphics for the human perceptual system, we must understand, at a basic level, the makeup of the perceptual system. There are multiple levels of perception that must correctly function in order to perceive visual stimuli successfully, but a somewhat simplistic higher-level analogy would be that we must understand both the hardware and software of the human visual system to create effective graphics.</p>
<p>The “hardware”, in this analogy, consists of the neurons that make up the eyes, optic nerve, and the brain itself. The higher-level functions (object recognition, working memory, etc.) comprise the “software” component. In addition, much like computer software, there are different programs running simultaneously; these programs may interact with each other, run sequentially, or run in parallel. Here, we provide an overview of the important components of the visual system that influence graphical perception. First, we discuss the grey-matter (hardware) components of the visual system, then we examine the higher-level cognitive heuristics (software) that order the raw input and construct our visual environment.</p>
<section id="hardware" class="level2">
<h2 class="anchored" data-anchor-id="hardware">Hardware</h2>
<p>The physiology of perception is complex; what follows is a brief overview of the physiology of perception, focusing on the areas most important to the perception of statistical graphics. It is important to distinguish between the sensation (the retinal image) and the perception (the corresponding mental representation) of an object. This overview will entirely ignore the finer details of the organization of the brain: feature detector cells, specific processing units for certain types of visual stimuli, and most of the experiments and incidents that led to our current understanding of how the brain processes visual information. A more thorough presentation of these aspects of perception can be found in <span class="citation" data-cites="goldsteinSensationPerception2022">Goldstein and Cacciamani (<a href="#ref-goldsteinSensationPerception2022" role="doc-biblioref">2022</a>)</span>.</p>
<section id="the-eye" class="level3">
<h3 class="anchored" data-anchor-id="the-eye">The Eye</h3>
<p>The eye is a complex apparatus, but for our purposes, the most important component of the eye is the retina, which contains the sensory cells responsible for transforming light waves into electrical information in the form of neural signals. These sensory cells are specialized neurons, known as rods and cones, which perceive light intensity (brightness) and wavelength (color), respectively. One section of the retina, known as the fovea, contains only cones; the rest of the retina contains a mixture of rods and cones. <a href="#fig-retina-diagram" class="quarto-xref">Figure&nbsp;1</a> depicts the structure of the eye with a closeup of the retina.</p>
<div id="fig-retina-diagram" class="quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-retina-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://upload.wikimedia.org/wikipedia/commons/9/9e/Figure_36_05_02.png" class="img-fluid figure-img">
</div>
<figcaption class="figure quarto-float-caption quarto-float-fig" id="fig-retina-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: The human eye, with closeup of receptor cells in the retina. <span class="fig-citation"><a href="https://commons.wikimedia.org/wiki/File:Figure_36_05_02.png">Image Source</a> <a href="https://creativecommons.org/licenses/by/4.0/deed.en">License</a> Authors: OpenStax Copyright Holders: Rice University Publishers: OpenStax OpenStax Biology</span>
</figcaption>
</figure>
</div>
<p>Another important region of the retina is the blindspot, the area where the optic nerve exits the eye to connect the retina to the brain. There are no rods or cones in this region of the retina, and any vision in the region of space that maps onto this point is a result of two mechanisms: binocular vision (the other eye fills in the missing information) and your brain “filling in” what it believes should be there.</p>
<div id="fig-color-range" class="quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-color-range-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../fig/Absorption-Spectra-Retina.svg" class="img-fluid figure-img">
</div>
<figcaption class="figure quarto-float-caption quarto-float-fig" id="fig-color-range-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Absorption spectra of rods and short (blue), medium (green), and long (red) wave cones. <span class="fig-citation">Image modified from <a href="https://commons.wikimedia.org/wiki/File:Spectre_absorption_des_cones.svg">Pancrat’s Wikimedia Commons work</a>. <a href="https://creativecommons.org/licenses/by-sa/3.0/deed.en">License</a>.</span>
</figcaption>
</figure>
</div>
<p><a href="#fig-color-range" class="quarto-xref">Figure&nbsp;2</a> shows the responsiveness of rods and each of the three types of cones to wavelengths of light in the visual spectrum. As a result of the response of cone cells to different wavelengths of light, humans with normal color vision can better distinguish colors in the yellow-green portion of the color spectrum compared to colors in the red or blue portions of the spectrum.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Implications for Color Schemes
</div>
</div>
<div class="callout-body-container callout-body">
<p>Rainbow-style color schemes are seldom appropriate for conveying numerical values, because the correspondence between perceived information and the displayed information is not accurately maintained by the visual system <span class="citation" data-cites="golebiowska2022a liuSomewhereRainbowEmpirical2018a borlandRainbowColorMap2007 lightEndRainbowColor2004">(<a href="#ref-golebiowska2022a" role="doc-biblioref">Golebiowska and Coltekin 2022</a>; <a href="#ref-liuSomewhereRainbowEmpirical2018a" role="doc-biblioref">Liu and Heer 2018</a>; <a href="#ref-borlandRainbowColorMap2007" role="doc-biblioref">Borland and Taylor 2007</a>; <a href="#ref-lightEndRainbowColor2004" role="doc-biblioref">Light and Bartlein 2004</a>)</span>. Rainbow schemes may perform slightly better in situations where the goal is to emphasize distributions of values <span class="citation" data-cites="redaRainbowColormapsWhat2022">(<a href="#ref-redaRainbowColormapsWhat2022" role="doc-biblioref">Reda 2022</a>)</span>, but this effect is small relative to the disadvantages of rainbow color schemes for accessibility and interpretability.</p>
<p>Moreover, if a viewer has any level of color vision impairment (colloquially, ‘color blindness’), the viewer cannot perceive the full spectrum of colors. An estimated 5% of the population (10% of males, less than 1% of females) has some form of color vision impairment. Rainbow color schemes perform particularly poorly when color vision is impaired.</p>
</div>
</div>
</section>
<section id="the-brain" class="level3">
<h3 class="anchored" data-anchor-id="the-brain">The Brain</h3>
<p>Once light hits the retina and causes a signal in the receptor cells, the information travels along the optic nerve and into the brain. Multiple neighboring rods are connected to the same neuron, where each cone is connected to a single neuron. The combined wiring of rod cells is responsible for the <a href="https://en.wikipedia.org/wiki/Grid_illusion#Theories">Hermann grid illusion</a> and the <a href="https://en.wikipedia.org/wiki/Mach_bands">Mach bands</a> seen in <a href="#fig-inhibition-illusions" class="quarto-xref">Figure&nbsp;3</a>. Both of these illusions are a product of lateral inhibition, which is a result of the wiring of rod cells in the retina. Essentially, neurons can only fire at a specific rate, so when neighboring cells are all stimulated simultaneously, the combined neuron cannot fire fast enough to pass on all of the signals, causing <em>inhibition</em>. The <a href="https://en.wikipedia.org/wiki/Grid_illusion#Theories">specifics of this response</a> and its relationship with the wiring of the receptor cells are too complex for this summary.</p>
<div id="fig-inhibition-illusions" class="quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-inhibition-illusions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-hermann-grid" class="quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-hermann-grid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://upload.wikimedia.org/wikipedia/commons/b/be/HermannGrid.svg" class="img-fluid figure-img" data-ref-parent="fig-inhibition-illusions">
</div>
<figcaption class="figure quarto-subfloat-caption quarto-subfloat-fig" id="fig-hermann-grid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Hermann grid illusion <span class="fig-citation"><a href="https://en.wikipedia.org/wiki/File:HermannGrid.svg">Image Source</a>. <a href="https://creativecommons.org/publicdomain/zero/1.0/deed.en">License</a>.</span>
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-mach-bands" class="quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-mach-bands-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../fig/Mach-Bands.png" class="img-fluid figure-img" data-ref-parent="fig-inhibition-illusions">
</div>
<figcaption class="figure quarto-subfloat-caption quarto-subfloat-fig" id="fig-mach-bands-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Mach Bands <span class="fig-citation"><a href="https://commons.wikimedia.org/wiki/File:Bandes_de_mach.PNG">Image Source</a>. <a href="https://en.wikipedia.org/wiki/en:Free_Art_License">License</a>.</span>
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
<figcaption class="figure quarto-float-caption quarto-float-fig" id="fig-inhibition-illusions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Illusions which are thought to arise due to inhibition. In the Hermann grid illusion, dark blobs appear at the intersection of the white lines. In the Mach band illusion, the region where different shaded bands meet seems to have more contrast than the middle of the bands.
</figcaption>
</figure>
</div>
<p>Once neural impulses have left the retina through the optic nerve, they travel to the visual cortex by way of several specialized structures within the brain that process lower-level signals. Receptor cells in the visual cortex respond to specific angles, spatial locations, colors, and intensities, and arrays of these special ‘feature detector cells’ process the information into a form used by higher-level processes <span class="citation" data-cites="hubelReceptiveFieldsBinocular1962">(<a href="#ref-hubelReceptiveFieldsBinocular1962" role="doc-biblioref">Hubel and Wiesel 1962</a>)</span>. These higher-level processes are what we have previously called ‘software’: they are not directly related to the physical brain, but they do process information heuristically to produce higher-level reasoning and conclusions. In the next section, we explore some of the higher-level processes responsible for visual perception.</p>
</section>
</section>
<section id="software" class="level2">
<h2 class="anchored" data-anchor-id="software">Software</h2>
<p>Many of the processes for visual perception run simultaneously; in absence of a strict temporal ordering, we will start with the more basic tasks of visual perception and proceed towards higher-level processes.</p>
<section id="selective-attention" class="level3">
<h3 class="anchored" data-anchor-id="selective-attention">Selective Attention</h3>
<p>In many tasks, it is necessary to pay attention to many parallel input streams simultaneously; this is particularly true for complex tasks like driving a car. These tasks demand divided attention; the brain must process many different sources of information in parallel. By contrast, most image recognition tasks require selective attention, that is, focusing on specific objects and ignoring everything else.</p>
<p>Selective attention is accomplished by focusing the fovea (the area with the highest visual acuity) on the object. For instance, if the object is a page of text, each word will pass through the fovea, producing a focused stream of visual input. This stream of input consists of saccades (jumps between points of focus) and pauses in which the visual information is relayed to the brain.</p>
<p>Selective attention is generally necessary for perception to occur, though there is some information that is encoded automatically. The <a href="http://www.theinvisiblegorilla.com/videos.html">“gorilla” film</a> experiment demonstrates that even when there is attention focused on a task, information extraneous to that task is not always encoded, that is, when participants focused on counting the number of passes between players in the basketball game, many did not notice the gorilla walking through the middle of the court. It is important to understand which parts of a visual stimulus are the focus of a given perceptual task, because most of the information encoded by the brain is a result of selective attention. Eye-tracking can be an important tool useful to understand these perceptual processes, but participants may also be able to self-report which parts of a stimulus contributed to their decision.</p>
<p>Within the brain, attention is important because it allows different regions of the brain which process color, shape, and position to integrate these perceptions into a multifaceted mental representation of the object <span class="citation" data-cites="goldsteinSensationPerception2022">(<a href="#ref-goldsteinSensationPerception2022" role="doc-biblioref">Goldstein and Cacciamani 2022</a>)</span>. This process, known as binding, is essential to coherently encode a scene into working memory. Feature integration theory <span class="citation" data-cites="treismanFeatureIntegrationTheoryAttention1980">(<a href="#ref-treismanFeatureIntegrationTheoryAttention1980" role="doc-biblioref">Treisman 1980</a>)</span> suggests that these separate streams of information are initially encoded in the preattentive stage of object perception; focusing on the object triggers the binding of these separate streams into a single coherent stream of information. Many single features, such as color, length, and texture are <em>preattentive</em>, because they can be pinpointed in an image without focused attention (and thus can be located faster), but specific combinations of color and shape require attention (because the features must be bound together) and are thus more difficult to search. Preattentive features are generally processed in parallel (that is, the entire scene is processed nearly simultaneously), while features requiring attention are processed serially.</p>
<div id="fig-parallel-serial" class="quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-parallel-serial-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>

</div>
<div class="cell quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="01-Fundamentals_files/figure-html/parallel-serial-1.png" class="img-fluid figure-img" width="384"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="01-Fundamentals_files/figure-html/parallel-serial-2.png" class="img-fluid figure-img" width="384"></p>
</div>
</div>
</div>
</div>
<figcaption class="figure quarto-float-caption quarto-float-fig" id="fig-parallel-serial-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Features detected serially or in parallel. In general, features detected in parallel are processed pre-attentively, while features detected serially require focused attention.
</figcaption>
</figure>
</div>
<p>Examples of features processed serially and in parallel are shown in <a href="#fig-parallel-serial" class="quarto-xref">Figure&nbsp;4</a> [<span class="citation" data-cites="helander1997handbook">Helander, Landauer, and Prabhu (<a href="#ref-helander1997handbook" role="doc-biblioref">1997</a>)</span>; Chapter 6].</p>
<p>Feature integration as a result of attention enables the brain to process a figure holistically and integrate all of the separate aspects of the object into a single perceptual experience. This processing is important for the most basic visual processes we take for granted, including object perception.</p>
</section>
<section id="object-perception" class="level3">
<h3 class="anchored" data-anchor-id="object-perception">Object Perception</h3>
<p>The most basic task of the visual system is to perceive objects in the world around us. This is an inherently difficult task, however, because the retina is a flat, two-dimensional surface responsible for conveying a three-dimensional visual scene. This dimensional reduction means that there are multiple three-dimensional stimuli that can produce the same visual image on the retina. This is known as the inverse projection problem - an infinite number of three-dimensional objects produce the same two-dimensional image. Less relevant to statistical graphics, but still complicating the object perception process, a single object can be viewed from a multitude of angles, in many different situations which may affect the retinal image (lighting, partial obstruction, etc). In addition, we recognize objects even when they are partially obscured or viewed from an angle we have not previously seen. These problems mean that the brain must utilize many different heuristics to increase the accuracy of the perceived world relative to an ambiguous stimulus.</p>
<p>The most commonly cited set of heuristics for object perception (and the set most relevant to statistical graphics) arise from the <em>Gestalt</em> school of psychology and are known as the <a href="https://en.wikipedia.org/wiki/Principles_of_grouping"><em>Principles of Grouping</em></a>. These principles relate to the idea ``the whole is greater than the sum of the parts’’, that is, that the components of a visual stimulus, when combined, create something that is more meaningful than the separate components considered individually. The Gestalt principles of grouping are as follows:</p>
<ul>
<li><strong>Pragnanz</strong> (the law of closure) Every stimulus pattern is seen so that the resulting structure is as simple as possible.</li>
<li><strong>Proximity</strong> Things that are close in space appear to be grouped.</li>
<li><strong>Similarity</strong> Similar items appear to be grouped together. The law of similarity is usually subordinate to the law of proximity.</li>
<li><strong>Good Continuation</strong> Points that can be connected to form straight lines or smooth curves seem to belong together, and lines seem to follow the smoothest path.</li>
<li><strong>Common Fate</strong> Things moving in the same direction are part of a single group.</li>
<li><strong>Familiarity</strong> Things are more likely to form groups if the groups are familiar.</li>
<li><strong>Common Region</strong> Things that are in the same region (container) appear to be grouped together</li>
<li><strong>Uniform Connectedness</strong> A connected region of objects is perceived as a single unit.</li>
<li><strong>Synchrony</strong> Events occurring at the same time will be perceived as belonging together.</li>
</ul>
<p>These principles are demonstrated in <a href="#fig-gestalt" class="quarto-xref">Figure&nbsp;5</a>.</p>
<div id="fig-gestalt" class="quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gestalt-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../fig/gestalt.svg" class="img-fluid figure-img">
</div>
<figcaption class="figure quarto-float-caption quarto-float-fig" id="fig-gestalt-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Gestalt principles of grouping
</figcaption>
</figure>
</div>
</section>
<section id="memory" class="level3">
<h3 class="anchored" data-anchor-id="memory">Memory</h3>
<p><span class="citation" data-cites="miller1956magical">Miller (<a href="#ref-miller1956magical" role="doc-biblioref">1956</a>)</span> suggested that active memory can contain only 7 (plus or minus two) chunks of information. A chunk of information could be a single letter or number, a meaningful collection of several letters or numbers (e.g.&nbsp;a word or an area code), or an association. This limitation is important in designing information for graphical consumption. For instance, the number of categories in legends should be limited to 7, to allow a viewer to store the associations within the legend and then use that information to understand the graph. Abuse of this limitation is referred to as a “color mapping attack” in <span class="citation" data-cites="conti2005attacking">Conti, Ahamad, and Stasko (<a href="#ref-conti2005attacking" role="doc-biblioref">2005</a>)</span>, a paper detailing the various ways to “attack” a human visualization system. Similarly, viewers should not be expected to remember more than 7 “chunks” of information from a single graph. Due to these limitations in memory, when a single color scale is used to represent more than one order of magnitude of variation, using a logarithmic scale provides more optimal information scaling than using a linear color scale <span class="citation" data-cites="sun2012framework varshney2013we">(<a href="#ref-sun2012framework" role="doc-biblioref">Sun et al. 2012</a>; <a href="#ref-varshney2013we" role="doc-biblioref">Varshney and Sun 2013</a>)</span>.</p>
</section>
<section id="information-integration" class="level3">
<h3 class="anchored" data-anchor-id="information-integration">Information Integration</h3>
<p>Integrating multiple dimensions of information (or mentally combining multiple graphics) is another area which can strain the ability of the brain to utilize information effectively. Well-constructed graphs can help the brain to integrate information by connecting points across dimensions (through the use of regression lines, clustering, etc.), which creates “chunks” of information that can then be stored in memory in a more compressed format. Gestalt principles of grouping are useful heuristics in part because they help define how these chunks of information form. In chart design, creating chunks of information is useful because this allows people to draw conclusions from multiple sets of data across multiple dimensions <span class="citation" data-cites="gattis1996mapping">(<a href="#ref-gattis1996mapping" role="doc-biblioref">Gattis and Holyoak 1996</a>)</span>. Poorly created graphics may make this task harder or even promote the encoding of misleading chunks; for instance, data that is overplotted may obscure the important trend and may also produce chunks which lead to the wrong associations being stored in memory. This integration limitation is very much related to short-term memory, but is also constrained by mental effort limitations and processing capacity. As a result, it is important to reduce the effort required to integrate multiple graphics.</p>
</section>
<section id="resource-limitations" class="level3">
<h3 class="anchored" data-anchor-id="resource-limitations">Resource Limitations</h3>
<p>Human attention is limited; thus visualizations which do not focus attention on important aspects of the data are likely to confuse the reader.</p>
<blockquote class="blockquote">
<p>“The greatest value of a picture is when it forces us to notice what we never expected to see”. <span class="citation" data-cites="eda">(<a href="#ref-eda" role="doc-biblioref">Tukey 1977</a>)</span></p>
</blockquote>
<p>When there are too many salient features to notice anything in particular, attention is split too many ways to gain useful information from the picture. Graphics should present data in a controlled fashion, so that focused attention is rewarded with useful information taken from the graph. <span class="citation" data-cites="conti2005attacking">Conti, Ahamad, and Stasko (<a href="#ref-conti2005attacking" role="doc-biblioref">2005</a>)</span> describes graphs that do not follow this principle as “processing attacks”, in that the overload the “CPU” with needless calculations and mental manipulations that are ultimately futile to understanding the data.</p>
<p>The consequence of the limits of human perception and processing capacity is that there is a limited amount of information one can expect to portray graphically; thus graphics should be designed to most efficiently communicate information so that this cognitive overload does not occur. The next section presents studies which examine the perception of graphs and charts directly across a wide range of perceptual levels and experimental conditions.</p>
</section>
</section>
</section>
<section id="the-psychology-of-charts" class="level1">
<h1>The Psychology of Charts</h1>
<p>In this section, we’ll primarily focus on how the concepts introduced previously apply to statistical graphics. At first glance, the cognitive psychology introduced above may seem unrelated to graphics and perception; however, this could not be further from the truth. It is critical to understand and consider perception and cognition when creating graphics which facilitate easy comprehension of the underlying data in visual form.</p>
<p>Graphics are useful in part because they summarize information in a form which is easier to understand and mentally manipulate. Thus, graphics serve as a form of <strong>external cognition</strong> - pre-processing information in a form that is (or should be) easier to digest and understand.</p>
<section id="preattentive-perception" class="level2">
<h2 class="anchored" data-anchor-id="preattentive-perception">Preattentive Perception</h2>
<p>As discussed in <a href="#fig-parallel-serial" class="quarto-xref">Figure&nbsp;4</a>, some features are processed pre-attentively, in parallel, while some features require conscious attention. When choosing features for data display, viewers will have an easier time when the feature chosen is processed in parallel than if the same data is shown using a feature that is processed serially.</p>
<p>However, it is important not to overdo it! Combinations of preattentive features used to show different dimensions of the data are processed serially, requiring much more effort, in an effect known as <em>interference</em>.</p>
<div id="fig-preattentive" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-preattentive-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-preattentive" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-preattentive-1" class="quarto-figure quarto-figure-center anchored" width="672">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-preattentive-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01-Fundamentals_files/figure-html/fig-preattentive-1.png" class="img-fluid figure-img" data-ref-parent="fig-preattentive" width="672">
</div>
<figcaption class="figure quarto-subfloat-caption quarto-subfloat-fig" id="fig-preattentive-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Color
</figcaption>
</figure>
</div>
</div>
<div class="cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-preattentive" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-preattentive-2" class="quarto-figure quarto-figure-center anchored" width="672">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-preattentive-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01-Fundamentals_files/figure-html/fig-preattentive-2.png" class="img-fluid figure-img" data-ref-parent="fig-preattentive" width="672">
</div>
<figcaption class="figure quarto-subfloat-caption quarto-subfloat-fig" id="fig-preattentive-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Shape
</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-preattentive" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-preattentive-3" class="quarto-figure quarto-figure-center anchored" width="672">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-preattentive-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01-Fundamentals_files/figure-html/fig-preattentive-3.png" class="img-fluid figure-img" data-ref-parent="fig-preattentive" width="672">
</div>
<figcaption class="figure quarto-subfloat-caption quarto-subfloat-fig" id="fig-preattentive-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(c) Interference
</figcaption>
</figure>
</div>
</div>
<div class="cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-preattentive" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-preattentive-4" class="quarto-figure quarto-figure-center anchored" width="672">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-preattentive-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01-Fundamentals_files/figure-html/fig-preattentive-4.png" class="img-fluid figure-img" data-ref-parent="fig-preattentive" width="672">
</div>
<figcaption class="figure quarto-subfloat-caption quarto-subfloat-fig" id="fig-preattentive-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(d) Dual Encoding
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="figure quarto-float-caption quarto-float-fig" id="fig-preattentive-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Shape and color are detected preattentively, but when used to encode different information, the combination is no longer preattentive. When shape and color are used to encode the same information (dual encoding), the combination is detected preattentively and is more accessible to individuals with perceptual disabilities such as colorblindness.
</figcaption>
</figure>
</div>
<p>Interference is demonstrated in <a href="#fig-preattentive" class="quarto-xref">Figure&nbsp;6</a>; the different point in (a) and (b) is easy to pick out, but it is much harder in (c) to separate shape and color in order to pick out differences. This is because when color and shape are used to show different features, we must consider every combination of color and shape used and individually search for points matching that description – an operation which requires a lot of time and cognitive effort. Not all combinations of color and shape are problematic, however: (d) uses color and shape to show the same variable (<em>dual-encoding</em>), which is useful for individuals who are color blind or may have trouble perceiving shapes. The two preattentive features support each other when used in this way, making it even easier to pick out the one mismatched point than in conditions (a), (b), or (c).</p>
</section>
<section id="conscious-perception" class="level2">
<h2 class="anchored" data-anchor-id="conscious-perception">Conscious Perception</h2>
<p>While it is useful to understand the psychology of perception and the implications of preattentive perception for understanding how much cognitive effort an operation takes, analysts are more concerned about conscious perception that occurs with attention. We care about questions like:</p>
<ul>
<li>Which parts of the graph are the most useful for answering a question?</li>
<li>How is information from the graph combined with pre-existing knowledge?</li>
<li>How does a graph promote understanding of the underlying data?</li>
</ul>
<p>Several different types of models have been proposed to describe this process, but overall, “task models” and “integration models” are most consistent with available empirical evidence.</p>
<section id="task-based-processing" class="level3">
<h3 class="anchored" data-anchor-id="task-based-processing">Task Based Processing</h3>
<p>An example of task-based processing <span class="citation" data-cites="ratwani2008thinking">(<a href="#ref-ratwani2008thinking" role="doc-biblioref">Ratwani, Trafton, and Boehm-Davis 2008</a>)</span> is shown in <a href="#fig-task-steps" class="quarto-xref">Figure&nbsp;7</a>.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-task-steps" class="quarto-figure quarto-figure-center anchored" width="50%">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-task-steps-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01-Fundamentals_files/figure-html/fig-task-steps-1.png" class="img-fluid figure-img" style="width:50.0%">
</div>
<figcaption class="figure quarto-float-caption quarto-float-fig" id="fig-task-steps-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Task-based steps for evaluating the relationship between eruption length and time between eruptions for Old Faithful?
</figcaption>
</figure>
</div>
</div>
</div>
<p><strong>Question:</strong> What is the relationship between the length of the eruption and the time between eruptions for Old Faithful?</p>
<ol type="1">
<li><p>Understand the question:</p>
<ul>
<li>Identify “length of eruption” and “time between eruptions” as things to search for in the graph.</li>
</ul></li>
<li><p>Search for identified quantities:</p>
<ul>
<li>Look for “length of eruption” on the axes and determine that the <span class="math inline">\(y\)</span>-coordinate contains that information.</li>
<li>Look for “time between eruptions” on the axes and determine that the <span class="math inline">\(x\)</span>-coordinate contains that information.</li>
<li>Verify that these quantities are what are sought by re-reading the question.</li>
</ul></li>
<li><p>Sense-making and Storytelling:</p>
<ul>
<li>Establish that as the time between eruptions increases, the length of the eruption increases.</li>
<li>Note that there seems to be a bimodal distribution of points</li>
</ul></li>
<li><p>Answer the question:</p>
<ul>
<li>As time between eruptions increases, length of the eruption increases.</li>
</ul></li>
</ol>
<p>In practice, the search portion of the task based framework is implicitly connected to the sense-making and storytelling portion, and viewers iterate between the two steps several times before finally proceeding to step 4. The time required for each step may change based on the reader’s familiarity with the task, the chart style, and the background knowledge required to interpret the data. Viewers who are familiar with similar graphics may be able to encode information faster and in larger chunks, answering the question more quickly <span class="citation" data-cites="carpenter1998model">(<a href="#ref-carpenter1998model" role="doc-biblioref">Carpenter and Shah 1998</a>)</span>.</p>
</section>
<section id="information-integration-1" class="level3">
<h3 class="anchored" data-anchor-id="information-integration-1">Information Integration</h3>
<p>Charts designed to promote chunking do so by providing viewers with cues for important features. This can help participants come to conclusions supported by the data and statistical modeling underlying the visual representation, reducing cognitive load.</p>
<p><a href="#fig-grouping-help" class="quarto-xref">Figure&nbsp;8</a> shows the same data using three different representations, with an additional representation illustrating the viewer’s mental model. In the first figure, (a), viewers are provided with no additional information and must group points together mentally to make sense of the data; this grouping action takes some cognitive effort, producing something like the second figure, (b). The designer could make this chunking effort less resource-intensive by</p>
<div id="fig-grouping-help" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-grouping-help-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-grouping-help" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-grouping-help-1" class="quarto-figure quarto-figure-center anchored" width="672">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-grouping-help-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01-Fundamentals_files/figure-html/fig-grouping-help-1.png" class="img-fluid figure-img" data-ref-parent="fig-grouping-help" width="672">
</div>
<figcaption class="figure quarto-subfloat-caption quarto-subfloat-fig" id="fig-grouping-help-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Data with no additional aesthetics
</figcaption>
</figure>
</div>
</div>
<div class="cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-grouping-help" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-grouping-help-2" class="quarto-figure quarto-figure-center anchored" width="672">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-grouping-help-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01-Fundamentals_files/figure-html/fig-grouping-help-2.png" class="img-fluid figure-img" data-ref-parent="fig-grouping-help" width="672">
</div>
<figcaption class="figure quarto-subfloat-caption quarto-subfloat-fig" id="fig-grouping-help-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Perceived chunks in mental representation
</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-grouping-help" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-grouping-help-3" class="quarto-figure quarto-figure-center anchored" width="672">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-grouping-help-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01-Fundamentals_files/figure-html/fig-grouping-help-3.png" class="img-fluid figure-img" data-ref-parent="fig-grouping-help" width="672">
</div>
<figcaption class="figure quarto-subfloat-caption quarto-subfloat-fig" id="fig-grouping-help-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(c) Data, color used to provide chunking information
</figcaption>
</figure>
</div>
</div>
<div class="cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-grouping-help" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-grouping-help-4" class="quarto-figure quarto-figure-center anchored" width="672">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-grouping-help-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01-Fundamentals_files/figure-html/fig-grouping-help-4.png" class="img-fluid figure-img" data-ref-parent="fig-grouping-help" width="672">
</div>
<figcaption class="figure quarto-subfloat-caption quarto-subfloat-fig" id="fig-grouping-help-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(d) Data, color + shape used to provide chunking information
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="figure quarto-float-caption quarto-float-fig" id="fig-grouping-help-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: When graphical features such as clustering are important, using aesthetics to promote chunking of the information can help readers make sense of the data more efficiently.
</figcaption>
</figure>
</div>
<p>Analyzing graphs using task-based models emphasizes the importance of spatial relationships between graphical elements. The gestalt laws of proximity and similarity dictate that items which are close together or physically similar (the same shape or color) are perceived as a group; this spatial perception creates “chunks” of the graph which may be encoded as single objects and thus reduce the mental bandwidth necessary to process the image. <a href="#fig-grouping-help" class="quarto-xref">Figure&nbsp;8</a> shows the advantage of “chunks” in graphs: in (a) there are 120 points that could be grouped into the three clusters shown in (b), but when the clustering is provided using aesthetics, the Gestalt similarity heuristic naturally groups the points for us, without much additional labor.</p>
<p>As charts become more complex, it can be difficult to consider all of the perceptual processes which might affect perception and use of a specific visualization. In these cases, cognitive models and frameworks can be extremely useful <span class="citation" data-cites="padillaCaseCognitiveModels2018 padillaDecisionMakingVisualizations2018">(<a href="#ref-padillaCaseCognitiveModels2018" role="doc-biblioref">Padilla 2018</a>; <a href="#ref-padillaDecisionMakingVisualizations2018" role="doc-biblioref">Padilla et al. 2018</a>)</span>. By assessing the interactions between the visualization and the cognitive model, it may be possible to identify potential areas of difficulty that can be addressed in a revised design.</p>
<p>Of graphics that present information of similar complexity, graphics that require less effort to understand and search for relevant information are preferable <span class="citation" data-cites="clevelandElementsGraphingData1985">(<a href="#ref-clevelandElementsGraphingData1985" role="doc-biblioref">William S. Cleveland 1985</a>)</span>. More complex models of the graphical perception process suggest that data are integrated on a visual level and then on a cognitive level, to form successive clusters of information. Once these clusters are formed, additional information can be integrated by comparing and contrasting different clusters to understand the higher-level meaning in the graph <span class="citation" data-cites="ratwani2008thinking">(<a href="#ref-ratwani2008thinking" role="doc-biblioref">Ratwani, Trafton, and Boehm-Davis 2008</a>)</span>. Graph types which cater to this hierarchical clustering mechanism may be more easily understood by viewers than graphs that do not provide information in a manner easily assimilated by the human brain. Facetted charts may be particularly useful for mapping multidimensional data to provide “chunks” of information in a relevant manner, pre-digested for integration into the viewer’s working conceptual understanding of the dataset. Additionally, color schemes and appropriate labeling of graph features which reduce the amount of work necessary to integrate numerical information from a legend into the visual representation of the graph facilitate graphical inference <span class="citation" data-cites="carpenter1998model">(<a href="#ref-carpenter1998model" role="doc-biblioref">Carpenter and Shah 1998</a>)</span>.</p>
</section>
</section>
<section id="simple-charts" class="level2">
<h2 class="anchored" data-anchor-id="simple-charts">Simple Charts</h2>
<p>A series of experiments by <span class="citation" data-cites="clevelandGraphicalPerceptionTheory1984 clevelandGraphicalPerceptionGraphical1985 clevelandGraphicalPerceptionVisual1987">(<a href="#ref-clevelandGraphicalPerceptionTheory1984" role="doc-biblioref">William S. Cleveland and McGill 1984</a>, <a href="#ref-clevelandGraphicalPerceptionGraphical1985" role="doc-biblioref">1985</a>, <a href="#ref-clevelandGraphicalPerceptionVisual1987" role="doc-biblioref">1987</a>)</span> examined basic perceptual tasks, establishing the relative accuracy of comparisons made using different graphical elements. This ranking is shown in <a href="#tbl-task-hierarchy" class="quarto-xref">Table&nbsp;1</a>. Other researchers <span class="citation" data-cites="carswellChoosingSpecifiersEvaluation1992">(<a href="#ref-carswellChoosingSpecifiersEvaluation1992" role="doc-biblioref">C. M. Carswell 1992</a>)</span> have examined a wide range of studies beyond those conducted by Cleveland &amp; McGill and used meta-analysis to collapse this ranking into position/length/angle and area/volume, as the difference in accuracy between categories 1, 2, and 3 is small compared to categories 4 and 5.</p>
<div id="tbl-task-hierarchy" class="anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="table quarto-float-caption quarto-float-tbl" id="tbl-task-hierarchy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Cleveland &amp; McGill’s ordering of graphical tasks by accuracy. Higher ranking tasks are easier for viewers than low-ranking tasks and should be preferred in graphical design.
</figcaption>
<div aria-describedby="tbl-task-hierarchy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table">
<thead>
<tr class="header">
<th>Rank</th>
<th>Task</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Position (common scale)</td>
</tr>
<tr class="even">
<td>2</td>
<td>Position (non-aligned scale)</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Length, Direction, Angle, Slope</td>
</tr>
<tr class="even">
<td>4</td>
<td>Area</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Volume, Density, Curvature</td>
</tr>
<tr class="even">
<td>6</td>
<td>Shading, Color Saturation, Color Hue</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>It is important to note that the task asked of participants and the type of chart are both important: when presented with a line graph, viewers are more likely to summarize the graph in terms of the slope of the trend line (even when the x-axis is discrete); when presented with a bar graph, viewers summarize the information using discrete comparisons <span class="citation" data-cites="carswell1987information shah2005cambridge">(<a href="#ref-carswell1987information" role="doc-biblioref">M. C. Carswell and Wickens 1987</a>; <a href="#ref-shah2005cambridge" role="doc-biblioref">Shah and Miyake 2005</a>)</span>. The task and the graph format interact to influence viewer perceptions, thus, when creating graphics, statisticians should match appropriate graphical formats to meaningful conclusions about the data.</p>
<section id="color-shape-and-discriminability" class="level3">
<h3 class="anchored" data-anchor-id="color-shape-and-discriminability">Color, Shape, and Discriminability</h3>
<p>While shading, color saturation, and color hue rank poorly in the task hierarchy, this does not mean that they should be used to display information. Rather, it is important to remember that the feature hierarchy in <a href="#tbl-task-hierarchy" class="quarto-xref">Table&nbsp;1</a> was assembled based on numerical estimation accuracy. Estimation accuracy is not the only purpose of a chart - in fact, in general, if estimation accuracy is the only goal, a table is a better representation. While viewers may not be able to correlate a specific color to a specific numerical value, color is capable of providing ordinal information (this point is more saturated than that point) and even order of magnitude level comparisons. As color is a pre-attentive feature processed in parallel, the use of color is a trade-off between numerical precision and cognitive resources (speed and working memory).</p>
<p>It is important to consider working memory when constructing graphical scales (particularly when utilizing a discrete scale for categorical data), but it is also important to consider feature selection and discriminability as well. Color is generally believed to be preferrable for representing strata on a scatterplot <span class="citation" data-cites="clevelandGraphicalPerceptionTheory1984">(<a href="#ref-clevelandGraphicalPerceptionTheory1984" role="doc-biblioref">William S. Cleveland and McGill 1984</a>)</span>, but <span class="citation" data-cites="lewandowsky1989discriminating">Lewandowsky and Spence (<a href="#ref-lewandowsky1989discriminating" role="doc-biblioref">1989</a>)</span> found that if color is not available or appropriate, shapes, intensity, or discriminable letters may be utilized without a significant decrease in accuracy.</p>
<p>Discriminable letters are those which do not share physical features such as closure and symmetry, such as the letters H, Q, and X; confusable letters, such as H, E, and F, are associated with significantly less accurate perception. <span class="citation" data-cites="demiralpLearningPerceptualKernels2014">Demiralp, Bernstein, and Heer (<a href="#ref-demiralpLearningPerceptualKernels2014" role="doc-biblioref">2014</a>)</span> synthesized experimental evaluations of stimuli to create “perceptual kernels” describing the perceived distance between values; multidimensional scaling of the resulting distance matrix produces four distinct groups of shapes which share features (triangles with various degrees of rotation, squares and diamonds, and non-convex shapes such as x, +, and *). This separation suggests that feature integration underlies many of the processing speed effects found in studies examining discrete palettes and scales: palettes which are composed of confusable shapes, letters, or colors will require more processing time (and decrease accuracy) compared with discriminable palettes.</p>
</section>
<section id="other-considerations" class="level3">
<h3 class="anchored" data-anchor-id="other-considerations">Other Considerations</h3>
<p>Other graph features can also influence viewer inferences: multiple studies suggest that our mental schematic for a graph is most consistent with a <span class="math inline">\(45^\circ\)</span> trend line <span class="citation" data-cites="clevelandShapeParameterTwoVariable1988 tverskyPerceptualConceptualFactors1989">(<a href="#ref-clevelandShapeParameterTwoVariable1988" role="doc-biblioref">William S. Cleveland, McGill, and McGill 1988</a>; <a href="#ref-tverskyPerceptualConceptualFactors1989" role="doc-biblioref">Tversky and Schiano 1989</a>)</span>. “Banking to <span class="math inline">\(45^\circ\)</span>” is a commonly-cited recommendation for optimal graphics (it is also quite old, according to <span class="citation" data-cites="wickham2013graphical">Wickham (<a href="#ref-wickham2013graphical" role="doc-biblioref">2013</a>)</span>).</p>
<p>Axis scale transformations can make it easier for viewers to spot outliers of data conforming to skewed distributions (though this does require some domain-specific knowledge of statistical distributions), and appropriately labeled graphs can reduce the working memory requirements by reducing the number of back-and-forth comparisons required to pass information into working memory <span class="citation" data-cites="shah2005cambridge">(<a href="#ref-shah2005cambridge" role="doc-biblioref">Shah and Miyake 2005</a>)</span>.</p>
</section>
</section>
<section id="complex-domain-specific-charts" class="level2">
<h2 class="anchored" data-anchor-id="complex-domain-specific-charts">Complex, Domain-Specific Charts</h2>
</section>
</section>
<section id="strategies-for-readability" class="level1">
<h1>Strategies for Readability</h1>
<section id="center-primary-comparisons" class="level2">
<h2 class="anchored" data-anchor-id="center-primary-comparisons">Center Primary Comparisons</h2>
</section>
<section id="reduce-cognitive-load" class="level2">
<h2 class="anchored" data-anchor-id="reduce-cognitive-load">Reduce Cognitive Load</h2>
</section>
</section>
<section id="testing-multiple-versions-of-a-chart" class="level1">
<h1>Testing Multiple Versions of a Chart</h1>
<p>http://homepage.stat.uiowa.edu/~luke/classes/STAT4580-2024/notes.html#mostly-data-visualization</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-borlandRainbowColorMap2007" class="csl-entry" role="listitem">
Borland, D., and R. M. Taylor. 2007. <span>“Rainbow <span>Color Map</span> (<span>Still</span>) <span>Considered Harmful</span>.”</span> <em>IEEE Computer Graphics and Applications</em> 27 (2): 14–17. <a href="https://doi.org/10.1109/mcg.2007.323435">https://doi.org/10.1109/mcg.2007.323435</a>.
</div>
<div id="ref-carpenter1998model" class="csl-entry" role="listitem">
Carpenter, Patricia A, and Priti Shah. 1998. <span>“A Model of the Perceptual and Conceptual Processes in Graph Comprehension.”</span> <em>Journal of Experimental Psychology: Applied</em> 4 (2): 75.
</div>
<div id="ref-carswellChoosingSpecifiersEvaluation1992" class="csl-entry" role="listitem">
Carswell, C Melody. 1992. <span>“Choosing Specifiers: An Evaluation of the Basic Tasks Model of Graphical Perception.”</span> <em>Human Factors</em> 34 (5): 535–54. <a href="https://doi.org/10.1177/001872089203400503">https://doi.org/10.1177/001872089203400503</a>.
</div>
<div id="ref-carswell1987information" class="csl-entry" role="listitem">
Carswell, Melody C, and Christopher D Wickens. 1987. <span>“Information Integration and the Object Display an Interaction of Task Demands and Display Superiority.”</span> <em>Ergonomics</em> 30 (3): 511–27.
</div>
<div id="ref-clevelandElementsGraphingData1985" class="csl-entry" role="listitem">
Cleveland, William S. 1985. <em>The Elements of Graphing Data</em>. Vol. 2. Wadsworth Advanced Books; Software Monterey, <span>CA</span>.
</div>
<div id="ref-clevelandShapeParameterTwoVariable1988" class="csl-entry" role="listitem">
Cleveland, William S., Marylyn E. McGill, and Robert McGill. 1988. <span>“The Shape Parameter of a Two-Variable Graph.”</span> <em>Journal of the American Statistical Association</em> 83 (402): 289–300. <a href="https://doi.org/10.1080/01621459.1988.10478598">https://doi.org/10.1080/01621459.1988.10478598</a>.
</div>
<div id="ref-clevelandGraphicalPerceptionTheory1984" class="csl-entry" role="listitem">
Cleveland, William S., and Robert McGill. 1984. <span>“Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Methods.”</span> <em>Journal of the American Statistical Association</em> 79 (387): 531–54. <a href="https://doi.org/10.1080/01621459.1984.10478080">https://doi.org/10.1080/01621459.1984.10478080</a>.
</div>
<div id="ref-clevelandGraphicalPerceptionGraphical1985" class="csl-entry" role="listitem">
———. 1985. <span>“Graphical Perception and Graphical Methods for Analyzing Scientific Data.”</span> <em>Science</em> 229 (4716): 828–33. <a href="https://doi.org/10.1126/science.229.4716.828">https://doi.org/10.1126/science.229.4716.828</a>.
</div>
<div id="ref-clevelandGraphicalPerceptionVisual1987" class="csl-entry" role="listitem">
———. 1987. <span>“Graphical Perception: The Visual Decoding of Quantitative Information on Graphical Displays of Data.”</span> <em>Journal of the Royal Statistical Society. Series A (General)</em> 150 (3): 192. <a href="https://doi.org/10.2307/2981473">https://doi.org/10.2307/2981473</a>.
</div>
<div id="ref-conti2005attacking" class="csl-entry" role="listitem">
Conti, Gregory, Mustaque Ahamad, and John Stasko. 2005. <span>“Attacking Information Visualization System Usability Overloading and Deceiving the Human.”</span> In <em>Proceedings of the 2005 Symposium on Usable Privacy and Security</em>, 89–100. ACM.
</div>
<div id="ref-demiralpLearningPerceptualKernels2014" class="csl-entry" role="listitem">
Demiralp, Ç., M. S. Bernstein, and J. Heer. 2014. <span>“Learning Perceptual Kernels for Visualization Design.”</span> <em><span>IEEE</span> Transactions on Visualization and Computer Graphics</em> 20 (12): 1933–43. <a href="https://doi.org/10.1109/TVCG.2014.2346978">https://doi.org/10.1109/TVCG.2014.2346978</a>.
</div>
<div id="ref-gattis1996mapping" class="csl-entry" role="listitem">
Gattis, Merideth, and Keith J Holyoak. 1996. <span>“Mapping Conceptual to Spatial Relations in Visual Reasoning.”</span> <em>Journal of Experimental Psychology: Learning, Memory, and Cognition</em> 22 (1): 231.
</div>
<div id="ref-goldsteinSensationPerception2022" class="csl-entry" role="listitem">
Goldstein, E. Bruce, and Laura Cacciamani. 2022. <em>Sensation and Perception</em>. Eleventh edition. <span>Australia</span>: <span>Cengage</span>.
</div>
<div id="ref-golebiowska2022a" class="csl-entry" role="listitem">
Golebiowska, I. M., and A. Coltekin. 2022. <span>“Rainbow Dash: <span>Intuitiveness</span>, Interpretability and Memorability of the Rainbow Color Scheme in Visualization.”</span> <em>IEEE Transactions on Visualization and Computer Graphics</em> 28, 07: 2722–33. <a href="https://doi.org/10.1109/TVCG.2020.3035823">https://doi.org/10.1109/TVCG.2020.3035823</a>.
</div>
<div id="ref-helander1997handbook" class="csl-entry" role="listitem">
Helander, M. G., T. K. Landauer, and P. V. Prabhu. 1997. <em>Handbook of Human-Computer Interaction</em>. Second. <span>North Holland</span>. <a href="http://gen.lib.rus.ec/book/index.php?md5=4941df4ad2ca3481b0dc4291667428b7">http://gen.lib.rus.ec/book/index.php?md5=4941df4ad2ca3481b0dc4291667428b7</a>.
</div>
<div id="ref-hubelReceptiveFieldsBinocular1962" class="csl-entry" role="listitem">
Hubel, D. H., and T. N. Wiesel. 1962. <span>“Receptive Fields, Binocular Interaction and Functional Architecture in the Cat’s Visual Cortex.”</span> <em>The Journal of Physiology</em> 160 (1): 106–54. <a href="https://doi.org/10.1113/jphysiol.1962.sp006837">https://doi.org/10.1113/jphysiol.1962.sp006837</a>.
</div>
<div id="ref-lewandowsky1989discriminating" class="csl-entry" role="listitem">
Lewandowsky, Stephan, and Ian Spence. 1989. <span>“Discriminating Strata in Scatterplots.”</span> <em>Journal of the American Statistical Association</em> 84 (407): 682–88.
</div>
<div id="ref-lightEndRainbowColor2004" class="csl-entry" role="listitem">
Light, Adam, and Patrick J Bartlein. 2004. <span>“The End of the Rainbow? <span>Color</span> Schemes for Improved Data Graphics.”</span> <em>Eos, Transactions American Geophysical Union</em> 85 (40): 385–91. <a href="https://doi.org/10.1029/2004EO400002">https://doi.org/10.1029/2004EO400002</a>.
</div>
<div id="ref-liuSomewhereRainbowEmpirical2018a" class="csl-entry" role="listitem">
Liu, Yang, and Jeffrey Heer. 2018. <span>“Somewhere <span>Over</span> the <span>Rainbow</span>: <span>An Empirical Assessment</span> of <span>Quantitative Colormaps</span>.”</span> In <em>Proceedings of the 2018 <span>CHI Conference</span> on <span>Human Factors</span> in <span>Computing Systems</span></em>, 1–12. <span>Montreal QC Canada</span>: <span>ACM</span>. <a href="https://doi.org/10.1145/3173574.3174172">https://doi.org/10.1145/3173574.3174172</a>.
</div>
<div id="ref-miller1956magical" class="csl-entry" role="listitem">
Miller, George A. 1956. <span>“The Magical Number Seven, Plus or Minus Two: Some Limits on Our Capacity for Processing Information.”</span> <em>Psychological Review</em> 63 (2): 81.
</div>
<div id="ref-padillaCaseCognitiveModels2018" class="csl-entry" role="listitem">
Padilla, Lace M. 2018. <span>“A Case for Cognitive Models in Visualization Research : Position Paper.”</span> In <em>2018 <span>IEEE</span> Evaluation and Beyond - Methodological Approaches for Visualization (<span>BELIV</span>)</em>, 69–77. <a href="https://doi.org/10.1109/BELIV.2018.8634267">https://doi.org/10.1109/BELIV.2018.8634267</a>.
</div>
<div id="ref-padillaDecisionMakingVisualizations2018" class="csl-entry" role="listitem">
Padilla, Lace M., Sarah H. Creem-Regehr, Mary Hegarty, and Jeanine K. Stefanucci. 2018. <span>“Decision Making with Visualizations: A Cognitive Framework Across Disciplines.”</span> <em>Cognitive Research: Principles and Implications</em> 3 (December): 29. <a href="https://doi.org/10.1186/s41235-018-0120-9">https://doi.org/10.1186/s41235-018-0120-9</a>.
</div>
<div id="ref-ratwani2008thinking" class="csl-entry" role="listitem">
Ratwani, Raj M, J Gregory Trafton, and Deborah A Boehm-Davis. 2008. <span>“Thinking Graphically: Connecting Vision and Cognition During Graph Comprehension.”</span> <em>Journal of Experimental Psychology: Applied</em> 14 (1): 36.
</div>
<div id="ref-redaRainbowColormapsWhat2022" class="csl-entry" role="listitem">
Reda, Khairi. 2022. <span>“Rainbow <span>Colormaps</span>: <span>What</span> Are They Good and Bad For?”</span> <em>IEEE Transactions on Visualization and Computer Graphics</em>, 1–15. <a href="https://doi.org/10.1109/TVCG.2022.3214771">https://doi.org/10.1109/TVCG.2022.3214771</a>.
</div>
<div id="ref-shah2005cambridge" class="csl-entry" role="listitem">
Shah, Priti, and Akira Miyake. 2005. <em>The Cambridge Handbook of Visuospatial Thinking</em>. Cambridge University Press.
</div>
<div id="ref-sun2012framework" class="csl-entry" role="listitem">
Sun, John Z, Grace I Wang, Vivek K Goyal, and Lav R Varshney. 2012. <span>“A Framework for Bayesian Optimality of Psychophysical Laws.”</span> <em>Journal of Mathematical Psychology</em> 56 (6): 495–501.
</div>
<div id="ref-treismanFeatureIntegrationTheoryAttention1980" class="csl-entry" role="listitem">
Treisman, Anne M. 1980. <span>“A <span>Feature-Integration Theory</span> of <span>Attention</span>.”</span> <em>Cognitive Psychology</em> 12: 97–136. <a href="https://doi.org/10.1016/0010-0285(80)90005-5">https://doi.org/10.1016/0010-0285(80)90005-5</a>.
</div>
<div id="ref-eda" class="csl-entry" role="listitem">
Tukey, John. 1977. <em>Exploratory Data Analysis</em>. Reading, Massachusetts: Addison-Wesley.
</div>
<div id="ref-tverskyPerceptualConceptualFactors1989" class="csl-entry" role="listitem">
Tversky, Barbara, and Diane J Schiano. 1989. <span>“Perceptual and Conceptual Factors in Distortions in Memory for Graphs and Maps.”</span> <em>Journal of Experimental Psychology: General</em> 118 (4): 387. <a href="https://doi.org/10.1037/0096-3445.118.4.387">https://doi.org/10.1037/0096-3445.118.4.387</a>.
</div>
<div id="ref-varshney2013we" class="csl-entry" role="listitem">
Varshney, Lav R, and John Z Sun. 2013. <span>“Why Do We Perceive Logarithmically?”</span> <em>Significance</em> 10 (1): 28–31.
</div>
<div id="ref-wickham2013graphical" class="csl-entry" role="listitem">
Wickham, Hadley. 2013. <span>“Graphical Criticism: Some Historical Notes.”</span> <em>Journal of Computational and Graphical Statistics</em> 22 (1): 38–44. <a href="https://doi.org/10.1080/10618600.2012.761140">https://doi.org/10.1080/10618600.2012.761140</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    const typesetMath = (el) => {
      if (window.MathJax) {
        // MathJax Typeset
        window.MathJax.typeset([el]);
      } else if (window.katex) {
        // KaTeX Render
        var mathElements = el.getElementsByClassName("math");
        var macros = [];
        for (var i = 0; i < mathElements.length; i++) {
          var texText = mathElements[i].firstChild;
          if (mathElements[i].tagName == "SPAN") {
            window.katex.render(texText.data, mathElements[i], {
              displayMode: mathElements[i].classList.contains('display'),
              throwOnError: false,
              macros: macros,
              fleqn: false
            });
          }
        }
      }
    }
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        for (let i = 0; i < 2; i++) {
          container.appendChild(note.children[i].cloneNode(true));
        }
        typesetMath(container);
        return container.innerHTML
      } else {
        typesetMath(note);
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      typesetMath(note);
      return note.innerHTML;
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>
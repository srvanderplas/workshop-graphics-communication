[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Workshop: Effective Graphics for Visual Communication with Data",
    "section": "",
    "text": "Time\nTitle\nDescription\n\n\n\n\n8:30 - 9:00 AM\nIntroductions and Setup\nInstructors and students introduce themselves and get computers set up.\n\n\n9:00 - 10:45 AM\nFundamentals of Graphical Communication\nThe interaction between cognitive processes and graphics: how do we ensure people can read our charts?\nSlides\n\n\n10:45 - 11:00 AM\nBreak\n\n\n\n11:00 - 12:00 PM\nExploratory Data Analysis\nStrategies for using graphics to generate insight about new data through rapid iteration.\nSlides\n\n\n12:00 - 1:00 PM\nLunch\n\n\n\n1:00 - 2:45 PM\nAccessibility: more than just alt-text\nAccessibility guidelines applied to graphics. We will discuss color contrast rules, ways to help make graphics accessible to colorblind individuals, those with vision acuity loss, and those who use screen readers.\nSlides\n\n\n2:45 - 3:00 PM\nBreak\n\n\n\n3:00 - 3:20 PM\nThe Rules… and when to break them\nWe’ve spent the day up to this point discussing tactics for creating good data visualizations, but sometimes, the rules get in the way of effective communication with a specific audience. When is it ok to break these rules?\nSlides\n\n\n3:20 - 3:30\nBreak\n\n\n\n3:30 - 5:00 PM\nWorkshop: Improving our graphics\nWorking individually or in groups, we will improve existing graphics in order to apply the ideas in this workshop. Individuals can bring their own graphics to workshop or use graphics we’ve provided."
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Workshop: Effective Graphics for Visual Communication with Data",
    "section": "",
    "text": "Time\nTitle\nDescription\n\n\n\n\n8:30 - 9:00 AM\nIntroductions and Setup\nInstructors and students introduce themselves and get computers set up.\n\n\n9:00 - 10:45 AM\nFundamentals of Graphical Communication\nThe interaction between cognitive processes and graphics: how do we ensure people can read our charts?\nSlides\n\n\n10:45 - 11:00 AM\nBreak\n\n\n\n11:00 - 12:00 PM\nExploratory Data Analysis\nStrategies for using graphics to generate insight about new data through rapid iteration.\nSlides\n\n\n12:00 - 1:00 PM\nLunch\n\n\n\n1:00 - 2:45 PM\nAccessibility: more than just alt-text\nAccessibility guidelines applied to graphics. We will discuss color contrast rules, ways to help make graphics accessible to colorblind individuals, those with vision acuity loss, and those who use screen readers.\nSlides\n\n\n2:45 - 3:00 PM\nBreak\n\n\n\n3:00 - 3:20 PM\nThe Rules… and when to break them\nWe’ve spent the day up to this point discussing tactics for creating good data visualizations, but sometimes, the rules get in the way of effective communication with a specific audience. When is it ok to break these rules?\nSlides\n\n\n3:20 - 3:30\nBreak\n\n\n\n3:30 - 5:00 PM\nWorkshop: Improving our graphics\nWorking individually or in groups, we will improve existing graphics in order to apply the ideas in this workshop. Individuals can bring their own graphics to workshop or use graphics we’ve provided."
  },
  {
    "objectID": "demo/penguins-obs-olli.html",
    "href": "demo/penguins-obs-olli.html",
    "title": "Observable Penguins with Olli",
    "section": "",
    "text": "data=FileAttachment(\"../data/penguins.csv\").csv({typed: true})\n\n\n\n\n\n\nIf we want to use Olli with Observable, we need to separate the plot command from the plot specification:\n\npenguinChart = ({\n  marks: [\n    Plot.dot(data, {x: \"bill_length_mm\", y: \"bill_depth_mm\", fill: \"species\"})\n  ],\n  color: {legend: true}\n})\n\n\n\n\n\n\n\n\nCode\n// Create plot with specification\nPlot.plot(penguinChart);\n\n\n\n\n\n\nFigure 1: ?(caption)\n\n\n\n\n// Create Olli adapters\nOlliAdapters.ObservablePlotAdapter(penguinChart).then((olliVisSpec) =&gt; {\ndocument\n.getElementById(\"fig-penguin-plot\")\n.append(olli(olliVisSpec));\n})"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Course Description",
    "section": "",
    "text": "Susan Vanderplas\nAssistant Professor, University of Nebraska Lincoln\nHeike Hofmann\nProfessor, Iowa State University\nEmily Robinson\nAssistant Professor, California Polytechnic University"
  },
  {
    "objectID": "about.html#instructors",
    "href": "about.html#instructors",
    "title": "Course Description",
    "section": "",
    "text": "Susan Vanderplas\nAssistant Professor, University of Nebraska Lincoln\nHeike Hofmann\nProfessor, Iowa State University\nEmily Robinson\nAssistant Professor, California Polytechnic University"
  },
  {
    "objectID": "about.html#target-audience",
    "href": "about.html#target-audience",
    "title": "Course Description",
    "section": "Target Audience",
    "text": "Target Audience\nThis course is intended for practicing statisticians in industry, government, or academia who are responsible for communicating results of statistical analyses and data to non-statisticians.\n\nPrerequisites\nAttendees should be able to read data in, clean it, and visualize it using the language of their choice.\nExamples will be provided using ggplot2 code in R and seaborn or matplotlib in python. Instructors can assist students with R and python code during the workshop and will attempt to help with others; we do not promise familiarity with all programming languages in common use for data science."
  },
  {
    "objectID": "about.html#description",
    "href": "about.html#description",
    "title": "Course Description",
    "section": "Description",
    "text": "Description\nThis course will focus on strategies for creating data visualizations which make it easy for collaborators to gain insight from data. We will discuss different ways graphics are used during the analysis process, but will primarily focus on graphics used to communicate with non-statisticians: managers, stakeholders, and collaborators who may need to use graphics to make decisions and/or motivate changes. This course will also touch on topics such as accessibility and alt-text that are essential to ensuring that graphics meet regulatory requirements. The course will assume some familiarity with plotting packages such as base R graphics, ggplot2, seaborn, and/or matplotlib, but is not a “how to make graphics” course. Code for different plotting libraries will be provided and modified during the course."
  },
  {
    "objectID": "content/03-accessibility.html",
    "href": "content/03-accessibility.html",
    "title": "Accessibility: More than just Alt-text",
    "section": "",
    "text": "Graphics are one of the primary tools for science or data communication, and are powerful because they make use of our visual system in a way that off-loads much of the work of processing the data, freeing cognitive resources up to consider the content rather than the representation. Unfortunately, not everyone can leverage their visual systems in this way, due to differences in the visual system or in information processing systems within the brain. There are a wide range of issues which may impact how effectively people can use graphics, including colorblindness, poor visual acuity, blindness, dyslexia, dyscalculia, and even differences in data literacy and numerical literacy. In some sense, it can be useful to go through the introspection process when looking at a graph, and consider what perceptual and cognitive resources are required to complete each step when looking at a chart. Ultimately, as scientists and people working with data, we need to work to make our data representations accessible. The specifics of which populations we focus on, and how we adapt existing representations (or create new ones) are based on the target audience(s), and will differ across different disciplines. For instance, if you are designing graphics to be used in Air Traffic Controller trainings, you likely do not need to accommodate Blind and Low Vision (BLV) individuals or even consider color-blindness. However, if you are creating graphics to be consumed by a general web audience, it is important to consider a range of visual impairments and accommodations.\nThis is an active area of research in data science, information visualization, and design. It is always useful to see what new solutions have come out recently, because there are new developments in this area on a regular basis."
  },
  {
    "objectID": "content/03-accessibility.html#discoverability",
    "href": "content/03-accessibility.html#discoverability",
    "title": "Accessibility: More than just Alt-text",
    "section": "Discoverability",
    "text": "Discoverability\n\nAlt Text\nWhen retrofitting an existing page for accessibility, it may not be possible to make charts and graphics fully accessible to individuals who are blind or using screen-readers. In these cases, it is important to write good alt-text for each graph that is intended to convey information (it is fine to skip alt-text for purely decorative images).\nGood alt-text is:\n\nconcise\naccurate\nrelevant\ncontext-dependent - the same image may require different alt-text depending on the broader context of the web page.\n\nGraphs are some of the hardest images to fully describe in alt-text, in part because providing the same information that the image provides may require thousands of words, full data tables, or other accommodations. The alt text field in HTML does not allow for paragraphs, line breaks, and other structural elements; as a result, it is often better to include a short description in the alt-text field and a longer description (or table) as part of the web page source. Linking the alt-text and the longer description together may facilitate keyboard navigation between the two, making the navigation process less cognitively intensive for screen reader users.\nThe BrailleR R package integrates with many common R plotting functions (including base graphics and ggplot2) and can generate some functional alt-text automatically.\n\n\n\n\n\n\nBrailleR alt-text demo\n\n\n\n\n\n\nlibrary(BrailleR)\n\ndata &lt;- read.csv(\"../data/penguins.csv\")\nlibrary(ggplot2)\nscatterplot &lt;- ggplot(data, \n                      aes(x = bill_length_mm, \n                          y = bill_depth_mm, \n                          color = species)) + \n  geom_point()\n\nscatterplot\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nThis is an untitled chart with no subtitle or caption.\nIt has x-axis 'bill_length_mm' with labels 40, 50 and 60.\nIt has y-axis 'bill_depth_mm' with labels 15.0, 17.5 and 20.0.\nThere is a legend indicating colour is used to show species, with 3 levels:\nAdelie shown as strong reddish orange colour, \nChinstrap shown as vivid yellowish green colour and \nGentoo shown as brilliant blue colour.\nThe chart is a set of 342 big solid circle points of which about 97% can be seen.\n\n\n\n\n\n\n\nPage Structure\nThe entire web-page structure is important to consider when designing to include screen-reader users. Sighted users may be able to take in the web page structure visually and determine which elements to focus on; screen reader users have to take in the page structure audibly, and in sequence. Some users describe it as “viewing a web page through a straw”.\nIt is important to provide a contextual overview first, and then provide specific data and details in a structured hierarchy. We can update the “information seeking mantra” of overview, zoom and filter, details on demand to “gist”, “supporting methods” (contextual information), and “details” (actual data content). The structure of the page needs to support understanding when navigated hierarchically. Information embedded in visual design themes (font sizes, colors, nesting, space) need to be explicitly accessible via aria- fields to be accessible to screen reader users.\n\nNavigating with a Screen Reader"
  },
  {
    "objectID": "content/03-accessibility.html#discriminability",
    "href": "content/03-accessibility.html#discriminability",
    "title": "Accessibility: More than just Alt-text",
    "section": "Discriminability",
    "text": "Discriminability\nIn addition to the information below, which includes some web references, there is a lovely series of Observable posts on accessibility, contrast, and color choice for data visualization. Check it out!\n\nColor selection\nWe’ll first approach color selection with color impairment (aka “colorblindness”, though most color impaired people can see some colors) in mind, though many of the considerations here factor into contrast considerations later. There are several approaches to accommodating color impairment:\n\nAvoid red and green combinations. This helps but is not sufficient, particularly for those who have trouble with red and blue, rather than green.\nUse palettes designed to be “colorblind-friendly”, such as David Nichol’s, Okabe-Ito’s, Paul Tol’s. Colorbrewer’s colorblind friendly palettes are less useful than these options.\nDesign your graphic so that it is functional in greyscale. This will make it safe for all types of color impairment.\nDual-encode colors with other attributes, such as shape or linetype.\n\nIt can be difficult to fully accommodate those with color impairment, particularly when working with graphics that use many different hues. Keep in mind that even people with full color vision cannot keep more than \\(7 \\pm 2\\) items in working memory - so using many different colors is problematic for everyone, not just for those with impaired color vision.\n\n\nContrast\nIt can be hard to see content that does not have much contrast against the background. People with low vision rely on contrast even more than the rest of the population; in addition, individuals with color impairment tend to rely on contrast cues to determine whether ambiguous colors are, in fact, different.\nW3C (World Wide Web Consortium) creates Web Content Accessibility Guidelines (WCAG) to provide a standard of accessible online content. These guidelines have recommendations for creating alt-text, how to ensure accessibility of different types of media, and standards for how to make content distinguishable.\nWCAG guidelines are provided on a scale from A (basic accessibility) to AAA (most accessible).\n\n\n\n\n\n\n(A) guidelines for distinguishability:\n\n\n\n\n\n\nColor is not used as the only visual means of conveying information, indicating an action, prompting a response, or distinguishing a visual element.\nAudio Control: If any audio on a Web page plays automatically for more than 3 seconds, either a mechanism is available to pause or stop the audio, or a mechanism is available to control audio volume independently from the overall system volume level.\n\n\n\n\n\n\n\n\n\n\n(AA) guidelines for distinguishability:\n\n\n\n\n\n\nContrast: The visual presentation of text and images of text has a contrast ratio of at least 4.5:1, except for the following:\n\nLarge Text: Large-scale text and images of large-scale text have a contrast ratio of at least 3:1;\nIncidental: Text or images of text that are part of an inactive user interface component, that are pure decoration, that are not visible to anyone, or that are part of a picture that contains significant other visual content, have no contrast requirement.\nLogo: Text that is part of a logo or brand name has no contrast requirement.\n\nResize Text: Except for captions and images of text, text can be resized without assistive technology up to 200 percent without loss of content or functionality.\nImages of Text: If the technologies being used can achieve the visual presentation, text is used to convey information rather than images of text except for the following:\n\nCustomizable: The image of text can be visually customized to the user’s requirements;\nEssential: A particular presentation of text is essential to the information being conveyed. Logotypes (text that is part of a logo or brand name) are considered essential.\n\nReflow: Content can be presented without loss of information or functionality, and without requiring scrolling in two dimensions for:\n\nVertical scrolling content at a width equivalent to 320 CSS pixels;\nHorizontal scrolling content at a height equivalent to 256 CSS pixels.\nExcept for parts of the content which require two-dimensional layout for usage or meaning.\n\nNon-text Contrast: The visual presentation of the following have a contrast ratio of at least 3:1 against adjacent color(s):\n\nUser Interface Components: Visual information required to identify user interface components and states, except for inactive components or where the appearance of the component is determined by the user agent and not modified by the author;\nGraphical Objects: Parts of graphics required to understand the content, except when a particular presentation of graphics is essential to the information being conveyed.\n\nText Spacing: In content implemented using markup languages that support the following text style properties, no loss of content or functionality occurs by setting all of the following and by changing no other style property:\n\nLine height (line spacing) to at least 1.5 times the font size;\nSpacing following paragraphs to at least 2 times the font size;\nLetter spacing (tracking) to at least 0.12 times the font size;\nWord spacing to at least 0.16 times the font size.\nException: Human languages and scripts that do not make use of one or more of these text style properties in written text can conform using only the properties that exist for that combination of language and script.\n\nContent on Hover or Focus: Where receiving and then removing pointer hover or keyboard focus triggers additional content to become visible and then hidden, the following are true:\n\nDismissible: A mechanism is available to dismiss the additional content without moving pointer hover or keyboard focus, unless the additional content communicates an input error or does not obscure or replace other content;\nHoverable: If pointer hover can trigger the additional content, then the pointer can be moved over the additional content without the additional content disappearing;\nPersistent: The additional content remains visible until the hover or focus trigger is removed, the user dismisses it, or its information is no longer valid.\nException: The visual presentation of the additional content is controlled by the user agent and is not modified by the author.\n\n\n\n\n\n\n\n\n\n\n\n(AAA) guidelines for distinguishability:\n\n\n\n\n\n\nEnhanced Contrast: The visual presentation of text and images of text has a contrast ratio of at least 7:1, except for the following:\n\nLarge Text: Large-scale text and images of large-scale text have a contrast ratio of at least 4.5:1;\nIncidental: Text or images of text that are part of an inactive user interface component, that are pure decoration, that are not visible to anyone, or that are part of a picture that contains significant other visual content, have no contrast requirement.\nLogotypes: Text that is part of a logo or brand name has no contrast requirement.\n\nLow or No Background Audio: For prerecorded audio-only content that (1) contains primarily speech in the foreground, (2) is not an audio CAPTCHA or audio logo, and (3) is not vocalization intended to be primarily musical expression such as singing or rapping, at least one of the following is true:\n\nNo Background: The audio does not contain background sounds.\nTurn Off: The background sounds can be turned off.\n20 dB: The background sounds are at least 20 decibels lower than the foreground speech content, with the exception of occasional sounds that last for only one or two seconds.\n\nVisual Presentation: For the visual presentation of blocks of text, a mechanism is available to achieve the following:\n\nForeground and background colors can be selected by the user.\nWidth is no more than 80 characters or glyphs (40 if CJK).\nText is not justified (aligned to both the left and the right margins).\nLine spacing (leading) is at least space-and-a-half within paragraphs, and paragraph spacing is at least 1.5 times larger than the line spacing.\nText can be resized without assistive technology up to 200 percent in a way that does not require the user to scroll horizontally to read a line of text on a full-screen window.\n\nImages of Text: Images of text are only used for pure decoration or where a particular presentation of text is essential to the information being conveyed.\n\n\n\n\nYou can use https://www.accessibilitychecker.org/ to check the compliance of your website.\nChartability is a set of heuristics for ensuring accessibility of data visualizations (and the pages that contain them). It’s created by BLV designers and is designed to help you locate accessibility barriers in data visualizations. They maintain an audit workbook that has tests that help identify design failures."
  },
  {
    "objectID": "content/03-accessibility.html#sonification",
    "href": "content/03-accessibility.html#sonification",
    "title": "Accessibility: More than just Alt-text",
    "section": "Sonification",
    "text": "Sonification\nThere are other methods of communicating data without relying primarily on visual methods and adapting those representations to remove reliance on vision. Zong et al. (2024) developed Umwelt, which allows for editing of multimodal data representations, providing some support for sonification and non-visual data communication. There are also methods for adapting existing visualizations to produce sonified equivalents, using R tools like sonify or Python tools like Strauss, miditools (Russo 2024).\n\n\n\n\n\n\nSonifying Data With Python\n\n\n\n\n\nHere’s an example of how to create a data sonification using the penguins data, adapted from Russo (2024).\n\n# Code adapted from https://hub.ovh2.mybinder.org/user/systemsounds-so-ation-tutorials-vr3cdobo/doc/tree/data2midi-part1.ipynb\n\nimport pandas as pd\n\npenguins = pd.read_csv(\"../data/penguins.csv\").dropna()\n\n# Define a general mapping function\ndef map_value(value, min_value, max_value, min_result, max_result):\n  '''maps value (or array of values) from one range to another'''\n  \n  result = min_result + (value - min_value)/(max_value - min_value)*(max_result - min_result)\n  return result\n\n# Set desired duration: 15 seconds/beats\npenguins.bill_length_mm.describe # get info on penguin bill lengths\n\n&lt;bound method NDFrame.describe of 0      39.1\n1      39.5\n2      40.3\n4      36.7\n5      39.3\n       ... \n339    55.8\n340    43.5\n341    49.6\n342    50.8\n343    50.2\nName: bill_length_mm, Length: 333, dtype: float64&gt;\n\npenguins = penguins.sort_values(by=['species', 'bill_length_mm'], ascending = True) # sort data by bill length\n\nduration_beats = 15\nbpm = 60\nduration_sec = duration_beats*60/bpm\n\n# Scale x axis\npenguins[\"t_data\"] = map_value(penguins.bill_length_mm, min(penguins.bill_length_mm), max(penguins.bill_length_mm), 0, duration_beats)\n\n# Scale y axis\npenguins[\"y_data\"] = map_value(penguins.bill_depth_mm, min(penguins.bill_depth_mm), max(penguins.bill_depth_mm), 0, 1)\n\n# May want to transform data a bit - example uses sqrt\n# y_scale = 0.5\n# \n# penguins[\"y_data\"] = penguins.y_data**y_scale\n\nimport matplotlib.pyplot as plt\n\nplt.scatter(penguins.t_data, penguins.y_data)\nplt.xlabel('time (bill length, mm, normalized)')\nplt.ylabel('bill depth, mm, normalized')\nplt.show()\n\n\n\nplt.clf()\n\n# Scale penguin species\npenguins[\"track\"] = penguins.species.replace({\"Adelie\":0,\"Gentoo\":1,\"Chinstrap\":2})\nfor track_i in range(3):\n  dat = penguins.query(\"track==@track_i\")\n  plt.scatter(dat.t_data, dat.y_data)\n\nplt.xlabel('time (bill length, mm, normalized)')\nplt.ylabel('bill depth, mm, normalized')\nplt.show()\n\n\n\n\nNow that we’ve transformed the data, we can map data values to tones. MIDI tones and velocities are integers, so we must transform and then round our values to match the requirements of the medium we’re using.\n\n# In this case, I'm happy with just 2 octaves of chromatic notes, from C3 to C5. \n# These correspond to MIDI notes 48:72\n\npenguins[\"midi_y\"] = round(map_value(penguins.y_data, 0, 1, 48, 72))\npenguins[\"midi_y\"] = penguins.midi_y.convert_dtypes()\nfor track_i in range(3):\n  dat = penguins.query(\"track==@track_i\")\n  plt.scatter(dat.t_data, dat.midi_y)\nplt.xlabel('time (bill length, mm, normalized)')\nplt.ylabel('MIDI note number')\nplt.show()\n\n# Map data to note velocity - velocity is a combination of volume and intensity\n# We dual-encode pitch and velocity\n# \n# vel_min, vel_max = 35, 127\n# penguins[\"midi_vel\"] = round(map_value(penguins.y_data, 0, 1, vel_max, vel_min))\n# penguins[\"midi_vel\"] = penguins.midi_vel.convert_dtypes()\n\n\n\n\nFinally, we create the midi file. We add 3 tracks, one for each species - this would allow us to change the “program” (instrument) to correspond to each species group. For the moment, I’ve commented the program changes out because the addition of instruments makes the end result sound like elementary school band warm-up time (complete chaos).\n\n\nfrom midiutil.MidiFile import MIDIFile #import library to make midi file, https://midiutil.readthedocs.io/en/1.2.1/\n    \n#create midi file object, add tempo\nmy_midi_file = MIDIFile(3, deinterleave=False) #three tracks, one for each species \nmy_midi_file.addTempo(track=0, time=0, tempo=bpm) \nmy_midi_file.addTempo(track=1, time=0, tempo=bpm) \nmy_midi_file.addTempo(track=2, time=0, tempo=bpm) \n\n# \n# # .addProgramChange(track, channel, time, program)\n# my_midi_file.addProgramChange(0, 0, 0, 71) # first set of penguins as clarinet\n# my_midi_file.addProgramChange(1, 0, 0, 75) # second set of penguins as pan flute\n# my_midi_file.addProgramChange(2, 0, 0, 59) # third set of penguins as muted trumpets\n\n#add midi notes\nfor i in penguins.index:\n    my_midi_file.addNote(track=penguins.track[i], channel=0, pitch=penguins.midi_y[i], time=penguins.t_data[i], duration=0.25, volume=35)\n\n#create and save the midi file itself\nfilename = 'penguins_sonification.mid'\nwith open(filename, \"wb\") as f:\n    my_midi_file.writeFile(f) \n\n# Listen\n# import pygame #import library for playing midi files, https://pypi.org/project/pygame/\n# pygame.init()\n# pygame.mixer.music.load(filename)\n# pygame.mixer.music.play()\n\n\n\n\n\n\n\n\n\n\n\n\n\nResults of sonification of Figure 1 using python and MIDI audio encoding."
  },
  {
    "objectID": "content/03-accessibility.html#physicalization",
    "href": "content/03-accessibility.html#physicalization",
    "title": "Accessibility: More than just Alt-text",
    "section": "Physicalization",
    "text": "Physicalization\nThere are a number of ways to create accessible tactile charts using embossing machines, capsule paper (Brauner 2023), or 3D printers. Tactile graphics have higher performance than tactile tables or electronic tables accessed via screen reader (Watanabe and Mizukami 2018), in addition, tactile bar charts presented either alone or with auditory information have higher performance than audio-only presentation (Goncu, Marriott, and Hurst 2010).\nR packages like rayshader can be used to convert ggplot2 plots into 3D-printable STL files (Morgan-Wall 2024). This produces a STL file that has some tactile information, without requiring too much specialized software; it could be made more accessible by using a Braille font. One downside is that the height of the plot object is mapped to color/fill, and does not accommodate categorical mappings.\n\n\n\n\n\n\nRayshader demo\n\n\n\n\n\n\ndata &lt;- read.csv(\"../data/penguins.csv\")\nlibrary(ggplot2)\nplot &lt;- ggplot(data) + \n  stat_density_2d(aes(x = bill_length_mm, y = bill_depth_mm, \n                      fill = after_stat(!!str2lang(\"density\"))),\n                  contour = F, geom = \"raster\") +\n  scale_x_continuous(expand=c(0,0)) +\n  scale_y_continuous(expand=c(0,0))\n\nlibrary(rayshader)\nplot_gg(plot, emboss_text = .05)\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density2d()`).\nRemoved 2 rows containing non-finite outside the scale range\n(`stat_density2d()`).\n\nrgl::rglwidget() \n\n\n\n\n\n\nsave_3dprint(\"../fig/penguins-density-plot.stl\", maxwidth = 125, unit = \"mm\", rotate = TRUE)\n\nDimensions of model are: 125.0 mm x 125.0 mm x 25.0 mm"
  },
  {
    "objectID": "content/01-Fundamentals.html",
    "href": "content/01-Fundamentals.html",
    "title": "Fundamentals of Graphical Communication",
    "section": "",
    "text": "In order to design graphics for the human perceptual system, we must understand, at a basic level, the makeup of the perceptual system. There are multiple levels of perception that must correctly function in order to perceive visual stimuli successfully, but a somewhat simplistic higher-level analogy would be that we must understand both the hardware and software of the human visual system to create effective graphics.\nThe “hardware”, in this analogy, consists of the neurons that make up the eyes, optic nerve, and the brain itself. The higher-level functions (object recognition, working memory, etc.) comprise the “software” component. In addition, much like computer software, there are different programs running simultaneously; these programs may interact with each other, run sequentially, or run in parallel. Here, we provide an overview of the important components of the visual system that influence graphical perception. First, we discuss the grey-matter (hardware) components of the visual system, then we examine the higher-level cognitive heuristics (software) that order the raw input and construct our visual environment.\n\n\nThe physiology of perception is complex; what follows is a brief overview of the physiology of perception, focusing on the areas most important to the perception of statistical graphics. It is important to distinguish between the sensation (the retinal image) and the perception (the corresponding mental representation) of an object. This overview will entirely ignore the finer details of the organization of the brain: feature detector cells, specific processing units for certain types of visual stimuli, and most of the experiments and incidents that led to our current understanding of how the brain processes visual information. A more thorough presentation of these aspects of perception can be found in Goldstein and Cacciamani (2022).\n\n\nThe eye is a complex apparatus, but for our purposes, the most important component of the eye is the retina, which contains the sensory cells responsible for transforming light waves into electrical information in the form of neural signals. These sensory cells are specialized neurons, known as rods and cones, which perceive light intensity (brightness) and wavelength (color), respectively. One section of the retina, known as the fovea, contains only cones; the rest of the retina contains a mixture of rods and cones. Figure 1 depicts the structure of the eye with a closeup of the retina.\n\n\n\nFigure 1: The human eye, with closeup of receptor cells in the retina. Image Source License Authors: OpenStax Copyright Holders: Rice University Publishers: OpenStax OpenStax Biology\n\n\nAnother important region of the retina is the blindspot, the area where the optic nerve exits the eye to connect the retina to the brain. There are no rods or cones in this region of the retina, and any vision in the region of space that maps onto this point is a result of two mechanisms: binocular vision (the other eye fills in the missing information) and your brain “filling in” what it believes should be there.\n\n\n\nFigure 2: Absorption spectra of rods and short (blue), medium (green), and long (red) wave cones. Image modified from Pancrat’s Wikimedia Commons work. License.\n\n\nFigure 2 shows the responsiveness of rods and each of the three types of cones to wavelengths of light in the visual spectrum. As a result of the response of cone cells to different wavelengths of light, humans with normal color vision can better distinguish colors in the yellow-green portion of the color spectrum compared to colors in the red or blue portions of the spectrum.\n\n\n\n\n\n\nImplications for Color Schemes\n\n\n\nRainbow-style color schemes are seldom appropriate for conveying numerical values, because the correspondence between perceived information and the displayed information is not accurately maintained by the visual system (Golebiowska and Coltekin 2022; Liu and Heer 2018; Borland and Taylor 2007; Light and Bartlein 2004). Rainbow schemes may perform slightly better in situations where the goal is to emphasize distributions of values (Reda 2022), but this effect is small relative to the disadvantages of rainbow color schemes for accessibility and interpretability.\nMoreover, if a viewer has any level of color vision impairment (colloquially, ‘color blindness’), the viewer cannot perceive the full spectrum of colors. An estimated 5% of the population (10% of males, less than 1% of females) has some form of color vision impairment. Rainbow color schemes perform particularly poorly when color vision is impaired.\n\n\n\n\n\nOnce light hits the retina and causes a signal in the receptor cells, the information travels along the optic nerve and into the brain. Multiple neighboring rods are connected to the same neuron, where each cone is connected to a single neuron. The combined wiring of rod cells is responsible for the Hermann grid illusion and the Mach bands seen in Figure 3. Both of these illusions are a product of lateral inhibition, which is a result of the wiring of rod cells in the retina. Essentially, neurons can only fire at a specific rate, so when neighboring cells are all stimulated simultaneously, the combined neuron cannot fire fast enough to pass on all of the signals, causing inhibition. The specifics of this response and its relationship with the wiring of the receptor cells are too complex for this summary.\n\n\n\n\n\n\n\n\n(a) Hermann grid illusion Image Source. License.\n\n\n\n\n\n\n\n(b) Mach Bands Image Source. License.\n\n\n\n\n\nFigure 3: Illusions which are thought to arise due to inhibition. In the Hermann grid illusion, dark blobs appear at the intersection of the white lines. In the Mach band illusion, the region where different shaded bands meet seems to have more contrast than the middle of the bands.\n\n\nOnce neural impulses have left the retina through the optic nerve, they travel to the visual cortex by way of several specialized structures within the brain that process lower-level signals. Receptor cells in the visual cortex respond to specific angles, spatial locations, colors, and intensities, and arrays of these special ‘feature detector cells’ process the information into a form used by higher-level processes (Hubel and Wiesel 1962). These higher-level processes are what we have previously called ‘software’: they are not directly related to the physical brain, but they do process information heuristically to produce higher-level reasoning and conclusions. In the next section, we explore some of the higher-level processes responsible for visual perception.\n\n\n\n\nMany of the processes for visual perception run simultaneously; in absence of a strict temporal ordering, we will start with the more basic tasks of visual perception and proceed towards higher-level processes.\n\n\nIn many tasks, it is necessary to pay attention to many parallel input streams simultaneously; this is particularly true for complex tasks like driving a car. These tasks demand divided attention; the brain must process many different sources of information in parallel. By contrast, most image recognition tasks require selective attention, that is, focusing on specific objects and ignoring everything else.\nSelective attention is accomplished by focusing the fovea (the area with the highest visual acuity) on the object. For instance, if the object is a page of text, each word will pass through the fovea, producing a focused stream of visual input. This stream of input consists of saccades (jumps between points of focus) and pauses in which the visual information is relayed to the brain.\nSelective attention is generally necessary for perception to occur, though there is some information that is encoded automatically. The “gorilla” film experiment demonstrates that even when there is attention focused on a task, information extraneous to that task is not always encoded, that is, when participants focused on counting the number of passes between players in the basketball game, many did not notice the gorilla walking through the middle of the court. It is important to understand which parts of a visual stimulus are the focus of a given perceptual task, because most of the information encoded by the brain is a result of selective attention. Eye-tracking can be an important tool useful to understand these perceptual processes, but participants may also be able to self-report which parts of a stimulus contributed to their decision.\nWithin the brain, attention is important because it allows different regions of the brain which process color, shape, and position to integrate these perceptions into a multifaceted mental representation of the object (Goldstein and Cacciamani 2022). This process, known as binding, is essential to coherently encode a scene into working memory. Feature integration theory (Treisman 1980) suggests that these separate streams of information are initially encoded in the preattentive stage of object perception; focusing on the object triggers the binding of these separate streams into a single coherent stream of information. Many single features, such as color, length, and texture are preattentive, because they can be pinpointed in an image without focused attention (and thus can be located faster), but specific combinations of color and shape require attention (because the features must be bound together) and are thus more difficult to search. Preattentive features are generally processed in parallel (that is, the entire scene is processed nearly simultaneously), while features requiring attention are processed serially.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Features detected serially or in parallel. In general, features detected in parallel are processed pre-attentively, while features detected serially require focused attention.\n\n\nExamples of features processed serially and in parallel are shown in Figure 4 [Helander, Landauer, and Prabhu (1997); Chapter 6].\nFeature integration as a result of attention enables the brain to process a figure holistically and integrate all of the separate aspects of the object into a single perceptual experience. This processing is important for the most basic visual processes we take for granted, including object perception.\n\n\n\nThe most basic task of the visual system is to perceive objects in the world around us. This is an inherently difficult task, however, because the retina is a flat, two-dimensional surface responsible for conveying a three-dimensional visual scene. This dimensional reduction means that there are multiple three-dimensional stimuli that can produce the same visual image on the retina. This is known as the inverse projection problem - an infinite number of three-dimensional objects produce the same two-dimensional image. Less relevant to statistical graphics, but still complicating the object perception process, a single object can be viewed from a multitude of angles, in many different situations which may affect the retinal image (lighting, partial obstruction, etc). In addition, we recognize objects even when they are partially obscured or viewed from an angle we have not previously seen. These problems mean that the brain must utilize many different heuristics to increase the accuracy of the perceived world relative to an ambiguous stimulus.\nThe most commonly cited set of heuristics for object perception (and the set most relevant to statistical graphics) arise from the Gestalt school of psychology and are known as the Principles of Grouping. These principles relate to the idea ``the whole is greater than the sum of the parts’’, that is, that the components of a visual stimulus, when combined, create something that is more meaningful than the separate components considered individually. The Gestalt principles of grouping are as follows:\n\nPragnanz (the law of closure) Every stimulus pattern is seen so that the resulting structure is as simple as possible.\nProximity Things that are close in space appear to be grouped.\nSimilarity Similar items appear to be grouped together. The law of similarity is usually subordinate to the law of proximity.\nGood Continuation Points that can be connected to form straight lines or smooth curves seem to belong together, and lines seem to follow the smoothest path.\nCommon Fate Things moving in the same direction are part of a single group.\nFamiliarity Things are more likely to form groups if the groups are familiar.\nCommon Region Things that are in the same region (container) appear to be grouped together\nUniform Connectedness A connected region of objects is perceived as a single unit.\nSynchrony Events occurring at the same time will be perceived as belonging together.\n\nThese principles are demonstrated in Figure 5.\n\n\n\nFigure 5: Gestalt principles of grouping\n\n\n\n\n\nWe have discussed how visual stimuli are perceived and how objects are recognized; we now must examine how visual stimuli are encoded into memory. Most researchers believe that visual perceptions are encoded in an analog fashion, so that the memory of an image is closely related to the perception of that same image (cognition?). Other theories suggest that visual perceptions are encoded semantically, that is, the description of a visual scene would be encoded, rather than a mental ``image” of that scene. Both theories are likely at least partially correct, but the analog encoding of visual images is more relevant to statistical graphics because the accuracy of the stored image has the potential to affect recall of the contents of that image (and thus what people remember about a particular graphic). Experimental evidence for analog encoding includes the mental rotation task, where participants must determine whether or not a figure is a rotation of a target figure, as shown in Figure 6. (shepard1988mental?) showed that reaction time was proportional to the angle of rotation of the stimuli, which suggests that participants were mentally rotating the figure as they would rotate a three-dimensional figure in space.\n\n\n\nFigure 6: Rotation task from (shepard1988mental?). Are the two images the same? Image Source License\n\n\nIn addition, (kosslyn1978visual?) showed that mental representation of distances in a figure are accurate and that the time to encode those distances is proportional to the distances in the actual figure. These studies suggest that the memory of an image (statistical graphic or otherwise) is a reasonably accurate facsimile of the original image (though the accuracy of the mental representation is of course likely to be moderated by attention and recall ability).\nThe analog encoding of mental images is stored alongside the gist of an image. That is, a semantic (textual) description of the image is stored alongside its’ analog representation in memory. In some cases, recall ability is more consistent with the semantic encoding of images; that is, when shown an ambiguous figure and immediately asked for a description, participants could not give an alternate interpretation of the figure after the experiment was complete. In the case of figure Figure 7, participants who initially said the figure was a duck did not report having viewed a picture of a rabbit after the experiment, even though the image is consistent with either interpretation. This suggests that in some cases, verbal encoding of a figure (i.e. describing it as a duck) disrupts the mental representation of the picture. This is common in other types of memories as well: when the gist of a passage is stored, the actual content of the passage is no longer accessible. In other words, we would expect that if someone had to interpret a graph, they would remember the interpretation much more strongly than the actual graph, even if that interpretation was incorrect or incomplete.\n\n\n\nFigure 7: An ambiguous image that could be either a rabbit or a duck. When participants were asked to identify the image initially, they could not provide an alternate interpretation of the figure later. Redrawn by the author, inspired by\n\n\nThe “software” of the visual system much more complex than the few topics discussed here, but understanding attention, object perception, and how images are stored for later retrieval in the brain will make designing statistical graphics for the visual system easier and will also help with evaluating graphics based on the capabilities of the human visual system.\n\n\n\nMiller (1956) suggested that active memory can contain only 7 (plus or minus two) chunks of information. A chunk of information could be a single letter or number, a meaningful collection of several letters or numbers (e.g. a word or an area code), or an association. This limitation is important in designing information for graphical consumption. For instance, the number of categories in legends should be limited to 7, to allow a viewer to store the associations within the legend and then use that information to understand the graph. Abuse of this limitation is referred to as a “color mapping attack” in Conti, Ahamad, and Stasko (2005), a paper detailing the various ways to “attack” a human visualization system. Similarly, viewers should not be expected to remember more than 7 “chunks” of information from a single graph. Due to these limitations in memory, when a single color scale is used to represent more than one order of magnitude of variation, using a logarithmic scale provides more optimal information scaling than using a linear color scale (Sun et al. 2012; Varshney and Sun 2013).\n\n\n\nIntegrating multiple dimensions of information (or mentally combining multiple graphics) is another area which can strain the ability of the brain to utilize information effectively. Well-constructed graphs can help the brain to integrate information by connecting points across dimensions (through the use of regression lines, clustering, etc.), which creates “chunks” of information that can then be stored in memory in a more compressed format. Gestalt principles of grouping are useful heuristics in part because they help define how these chunks of information form. In chart design, creating chunks of information is useful because this allows people to draw conclusions from multiple sets of data across multiple dimensions (Gattis and Holyoak 1996). Poorly created graphics may make this task harder or even promote the encoding of misleading chunks; for instance, data that is overplotted may obscure the important trend and may also produce chunks which lead to the wrong associations being stored in memory. This integration limitation is very much related to short-term memory, but is also constrained by mental effort limitations and processing capacity. As a result, it is important to reduce the effort required to integrate multiple graphics.\n\n\n\nHuman attention is limited; thus visualizations which do not focus attention on important aspects of the data are likely to confuse the reader.\n\n“The greatest value of a picture is when it forces us to notice what we never expected to see”. (Tukey 1977)\n\nWhen there are too many salient features to notice anything in particular, attention is split too many ways to gain useful information from the picture. Graphics should present data in a controlled fashion, so that focused attention is rewarded with useful information taken from the graph. Conti, Ahamad, and Stasko (2005) describes graphs that do not follow this principle as “processing attacks”, in that the overload the “CPU” with needless calculations and mental manipulations that are ultimately futile to understanding the data.\nThe consequence of the limits of human perception and processing capacity is that there is a limited amount of information one can expect to portray graphically; thus graphics should be designed to most efficiently communicate information so that this cognitive overload does not occur. The next section presents studies which examine the perception of graphs and charts directly across a wide range of perceptual levels and experimental conditions."
  },
  {
    "objectID": "content/01-Fundamentals.html#hardware",
    "href": "content/01-Fundamentals.html#hardware",
    "title": "Fundamentals of Graphical Communication",
    "section": "",
    "text": "The physiology of perception is complex; what follows is a brief overview of the physiology of perception, focusing on the areas most important to the perception of statistical graphics. It is important to distinguish between the sensation (the retinal image) and the perception (the corresponding mental representation) of an object. This overview will entirely ignore the finer details of the organization of the brain: feature detector cells, specific processing units for certain types of visual stimuli, and most of the experiments and incidents that led to our current understanding of how the brain processes visual information. A more thorough presentation of these aspects of perception can be found in Goldstein and Cacciamani (2022).\n\n\nThe eye is a complex apparatus, but for our purposes, the most important component of the eye is the retina, which contains the sensory cells responsible for transforming light waves into electrical information in the form of neural signals. These sensory cells are specialized neurons, known as rods and cones, which perceive light intensity (brightness) and wavelength (color), respectively. One section of the retina, known as the fovea, contains only cones; the rest of the retina contains a mixture of rods and cones. Figure 1 depicts the structure of the eye with a closeup of the retina.\n\n\n\nFigure 1: The human eye, with closeup of receptor cells in the retina. Image Source License Authors: OpenStax Copyright Holders: Rice University Publishers: OpenStax OpenStax Biology\n\n\nAnother important region of the retina is the blindspot, the area where the optic nerve exits the eye to connect the retina to the brain. There are no rods or cones in this region of the retina, and any vision in the region of space that maps onto this point is a result of two mechanisms: binocular vision (the other eye fills in the missing information) and your brain “filling in” what it believes should be there.\n\n\n\nFigure 2: Absorption spectra of rods and short (blue), medium (green), and long (red) wave cones. Image modified from Pancrat’s Wikimedia Commons work. License.\n\n\nFigure 2 shows the responsiveness of rods and each of the three types of cones to wavelengths of light in the visual spectrum. As a result of the response of cone cells to different wavelengths of light, humans with normal color vision can better distinguish colors in the yellow-green portion of the color spectrum compared to colors in the red or blue portions of the spectrum.\n\n\n\n\n\n\nImplications for Color Schemes\n\n\n\nRainbow-style color schemes are seldom appropriate for conveying numerical values, because the correspondence between perceived information and the displayed information is not accurately maintained by the visual system (Golebiowska and Coltekin 2022; Liu and Heer 2018; Borland and Taylor 2007; Light and Bartlein 2004). Rainbow schemes may perform slightly better in situations where the goal is to emphasize distributions of values (Reda 2022), but this effect is small relative to the disadvantages of rainbow color schemes for accessibility and interpretability.\nMoreover, if a viewer has any level of color vision impairment (colloquially, ‘color blindness’), the viewer cannot perceive the full spectrum of colors. An estimated 5% of the population (10% of males, less than 1% of females) has some form of color vision impairment. Rainbow color schemes perform particularly poorly when color vision is impaired.\n\n\n\n\n\nOnce light hits the retina and causes a signal in the receptor cells, the information travels along the optic nerve and into the brain. Multiple neighboring rods are connected to the same neuron, where each cone is connected to a single neuron. The combined wiring of rod cells is responsible for the Hermann grid illusion and the Mach bands seen in Figure 3. Both of these illusions are a product of lateral inhibition, which is a result of the wiring of rod cells in the retina. Essentially, neurons can only fire at a specific rate, so when neighboring cells are all stimulated simultaneously, the combined neuron cannot fire fast enough to pass on all of the signals, causing inhibition. The specifics of this response and its relationship with the wiring of the receptor cells are too complex for this summary.\n\n\n\n\n\n\n\n\n(a) Hermann grid illusion Image Source. License.\n\n\n\n\n\n\n\n(b) Mach Bands Image Source. License.\n\n\n\n\n\nFigure 3: Illusions which are thought to arise due to inhibition. In the Hermann grid illusion, dark blobs appear at the intersection of the white lines. In the Mach band illusion, the region where different shaded bands meet seems to have more contrast than the middle of the bands.\n\n\nOnce neural impulses have left the retina through the optic nerve, they travel to the visual cortex by way of several specialized structures within the brain that process lower-level signals. Receptor cells in the visual cortex respond to specific angles, spatial locations, colors, and intensities, and arrays of these special ‘feature detector cells’ process the information into a form used by higher-level processes (Hubel and Wiesel 1962). These higher-level processes are what we have previously called ‘software’: they are not directly related to the physical brain, but they do process information heuristically to produce higher-level reasoning and conclusions. In the next section, we explore some of the higher-level processes responsible for visual perception."
  },
  {
    "objectID": "content/01-Fundamentals.html#software",
    "href": "content/01-Fundamentals.html#software",
    "title": "Fundamentals of Graphical Communication",
    "section": "",
    "text": "Many of the processes for visual perception run simultaneously; in absence of a strict temporal ordering, we will start with the more basic tasks of visual perception and proceed towards higher-level processes.\n\n\nIn many tasks, it is necessary to pay attention to many parallel input streams simultaneously; this is particularly true for complex tasks like driving a car. These tasks demand divided attention; the brain must process many different sources of information in parallel. By contrast, most image recognition tasks require selective attention, that is, focusing on specific objects and ignoring everything else.\nSelective attention is accomplished by focusing the fovea (the area with the highest visual acuity) on the object. For instance, if the object is a page of text, each word will pass through the fovea, producing a focused stream of visual input. This stream of input consists of saccades (jumps between points of focus) and pauses in which the visual information is relayed to the brain.\nSelective attention is generally necessary for perception to occur, though there is some information that is encoded automatically. The “gorilla” film experiment demonstrates that even when there is attention focused on a task, information extraneous to that task is not always encoded, that is, when participants focused on counting the number of passes between players in the basketball game, many did not notice the gorilla walking through the middle of the court. It is important to understand which parts of a visual stimulus are the focus of a given perceptual task, because most of the information encoded by the brain is a result of selective attention. Eye-tracking can be an important tool useful to understand these perceptual processes, but participants may also be able to self-report which parts of a stimulus contributed to their decision.\nWithin the brain, attention is important because it allows different regions of the brain which process color, shape, and position to integrate these perceptions into a multifaceted mental representation of the object (Goldstein and Cacciamani 2022). This process, known as binding, is essential to coherently encode a scene into working memory. Feature integration theory (Treisman 1980) suggests that these separate streams of information are initially encoded in the preattentive stage of object perception; focusing on the object triggers the binding of these separate streams into a single coherent stream of information. Many single features, such as color, length, and texture are preattentive, because they can be pinpointed in an image without focused attention (and thus can be located faster), but specific combinations of color and shape require attention (because the features must be bound together) and are thus more difficult to search. Preattentive features are generally processed in parallel (that is, the entire scene is processed nearly simultaneously), while features requiring attention are processed serially.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Features detected serially or in parallel. In general, features detected in parallel are processed pre-attentively, while features detected serially require focused attention.\n\n\nExamples of features processed serially and in parallel are shown in Figure 4 [Helander, Landauer, and Prabhu (1997); Chapter 6].\nFeature integration as a result of attention enables the brain to process a figure holistically and integrate all of the separate aspects of the object into a single perceptual experience. This processing is important for the most basic visual processes we take for granted, including object perception.\n\n\n\nThe most basic task of the visual system is to perceive objects in the world around us. This is an inherently difficult task, however, because the retina is a flat, two-dimensional surface responsible for conveying a three-dimensional visual scene. This dimensional reduction means that there are multiple three-dimensional stimuli that can produce the same visual image on the retina. This is known as the inverse projection problem - an infinite number of three-dimensional objects produce the same two-dimensional image. Less relevant to statistical graphics, but still complicating the object perception process, a single object can be viewed from a multitude of angles, in many different situations which may affect the retinal image (lighting, partial obstruction, etc). In addition, we recognize objects even when they are partially obscured or viewed from an angle we have not previously seen. These problems mean that the brain must utilize many different heuristics to increase the accuracy of the perceived world relative to an ambiguous stimulus.\nThe most commonly cited set of heuristics for object perception (and the set most relevant to statistical graphics) arise from the Gestalt school of psychology and are known as the Principles of Grouping. These principles relate to the idea ``the whole is greater than the sum of the parts’’, that is, that the components of a visual stimulus, when combined, create something that is more meaningful than the separate components considered individually. The Gestalt principles of grouping are as follows:\n\nPragnanz (the law of closure) Every stimulus pattern is seen so that the resulting structure is as simple as possible.\nProximity Things that are close in space appear to be grouped.\nSimilarity Similar items appear to be grouped together. The law of similarity is usually subordinate to the law of proximity.\nGood Continuation Points that can be connected to form straight lines or smooth curves seem to belong together, and lines seem to follow the smoothest path.\nCommon Fate Things moving in the same direction are part of a single group.\nFamiliarity Things are more likely to form groups if the groups are familiar.\nCommon Region Things that are in the same region (container) appear to be grouped together\nUniform Connectedness A connected region of objects is perceived as a single unit.\nSynchrony Events occurring at the same time will be perceived as belonging together.\n\nThese principles are demonstrated in Figure 5.\n\n\n\nFigure 5: Gestalt principles of grouping\n\n\n\n\n\nWe have discussed how visual stimuli are perceived and how objects are recognized; we now must examine how visual stimuli are encoded into memory. Most researchers believe that visual perceptions are encoded in an analog fashion, so that the memory of an image is closely related to the perception of that same image (cognition?). Other theories suggest that visual perceptions are encoded semantically, that is, the description of a visual scene would be encoded, rather than a mental ``image” of that scene. Both theories are likely at least partially correct, but the analog encoding of visual images is more relevant to statistical graphics because the accuracy of the stored image has the potential to affect recall of the contents of that image (and thus what people remember about a particular graphic). Experimental evidence for analog encoding includes the mental rotation task, where participants must determine whether or not a figure is a rotation of a target figure, as shown in Figure 6. (shepard1988mental?) showed that reaction time was proportional to the angle of rotation of the stimuli, which suggests that participants were mentally rotating the figure as they would rotate a three-dimensional figure in space.\n\n\n\nFigure 6: Rotation task from (shepard1988mental?). Are the two images the same? Image Source License\n\n\nIn addition, (kosslyn1978visual?) showed that mental representation of distances in a figure are accurate and that the time to encode those distances is proportional to the distances in the actual figure. These studies suggest that the memory of an image (statistical graphic or otherwise) is a reasonably accurate facsimile of the original image (though the accuracy of the mental representation is of course likely to be moderated by attention and recall ability).\nThe analog encoding of mental images is stored alongside the gist of an image. That is, a semantic (textual) description of the image is stored alongside its’ analog representation in memory. In some cases, recall ability is more consistent with the semantic encoding of images; that is, when shown an ambiguous figure and immediately asked for a description, participants could not give an alternate interpretation of the figure after the experiment was complete. In the case of figure Figure 7, participants who initially said the figure was a duck did not report having viewed a picture of a rabbit after the experiment, even though the image is consistent with either interpretation. This suggests that in some cases, verbal encoding of a figure (i.e. describing it as a duck) disrupts the mental representation of the picture. This is common in other types of memories as well: when the gist of a passage is stored, the actual content of the passage is no longer accessible. In other words, we would expect that if someone had to interpret a graph, they would remember the interpretation much more strongly than the actual graph, even if that interpretation was incorrect or incomplete.\n\n\n\nFigure 7: An ambiguous image that could be either a rabbit or a duck. When participants were asked to identify the image initially, they could not provide an alternate interpretation of the figure later. Redrawn by the author, inspired by\n\n\nThe “software” of the visual system much more complex than the few topics discussed here, but understanding attention, object perception, and how images are stored for later retrieval in the brain will make designing statistical graphics for the visual system easier and will also help with evaluating graphics based on the capabilities of the human visual system.\n\n\n\nMiller (1956) suggested that active memory can contain only 7 (plus or minus two) chunks of information. A chunk of information could be a single letter or number, a meaningful collection of several letters or numbers (e.g. a word or an area code), or an association. This limitation is important in designing information for graphical consumption. For instance, the number of categories in legends should be limited to 7, to allow a viewer to store the associations within the legend and then use that information to understand the graph. Abuse of this limitation is referred to as a “color mapping attack” in Conti, Ahamad, and Stasko (2005), a paper detailing the various ways to “attack” a human visualization system. Similarly, viewers should not be expected to remember more than 7 “chunks” of information from a single graph. Due to these limitations in memory, when a single color scale is used to represent more than one order of magnitude of variation, using a logarithmic scale provides more optimal information scaling than using a linear color scale (Sun et al. 2012; Varshney and Sun 2013).\n\n\n\nIntegrating multiple dimensions of information (or mentally combining multiple graphics) is another area which can strain the ability of the brain to utilize information effectively. Well-constructed graphs can help the brain to integrate information by connecting points across dimensions (through the use of regression lines, clustering, etc.), which creates “chunks” of information that can then be stored in memory in a more compressed format. Gestalt principles of grouping are useful heuristics in part because they help define how these chunks of information form. In chart design, creating chunks of information is useful because this allows people to draw conclusions from multiple sets of data across multiple dimensions (Gattis and Holyoak 1996). Poorly created graphics may make this task harder or even promote the encoding of misleading chunks; for instance, data that is overplotted may obscure the important trend and may also produce chunks which lead to the wrong associations being stored in memory. This integration limitation is very much related to short-term memory, but is also constrained by mental effort limitations and processing capacity. As a result, it is important to reduce the effort required to integrate multiple graphics.\n\n\n\nHuman attention is limited; thus visualizations which do not focus attention on important aspects of the data are likely to confuse the reader.\n\n“The greatest value of a picture is when it forces us to notice what we never expected to see”. (Tukey 1977)\n\nWhen there are too many salient features to notice anything in particular, attention is split too many ways to gain useful information from the picture. Graphics should present data in a controlled fashion, so that focused attention is rewarded with useful information taken from the graph. Conti, Ahamad, and Stasko (2005) describes graphs that do not follow this principle as “processing attacks”, in that the overload the “CPU” with needless calculations and mental manipulations that are ultimately futile to understanding the data.\nThe consequence of the limits of human perception and processing capacity is that there is a limited amount of information one can expect to portray graphically; thus graphics should be designed to most efficiently communicate information so that this cognitive overload does not occur. The next section presents studies which examine the perception of graphs and charts directly across a wide range of perceptual levels and experimental conditions."
  },
  {
    "objectID": "content/01-Fundamentals.html#preattentive-perception",
    "href": "content/01-Fundamentals.html#preattentive-perception",
    "title": "Fundamentals of Graphical Communication",
    "section": "Preattentive Perception",
    "text": "Preattentive Perception\nAs discussed in Figure 4, some features are processed pre-attentively, in parallel, while some features require conscious attention. When choosing features for data display, viewers will have an easier time when the feature chosen is processed in parallel than if the same data is shown using a feature that is processed serially.\nHowever, it is important not to overdo it! Combinations of preattentive features used to show different dimensions of the data are processed serially, requiring much more effort, in an effect known as interference.\n\n\n\n\n\n\n\n(a) Color\n\n\n\n\n\n\n\n(b) Shape\n\n\n\n\n\n\n\n\n\n(c) Interference\n\n\n\n\n\n\n\n(d) Dual Encoding\n\n\n\n\nFigure 8: Shape and color are detected preattentively, but when used to encode different information, the combination is no longer preattentive. When shape and color are used to encode the same information (dual encoding), the combination is detected preattentively and is more accessible to individuals with perceptual disabilities such as colorblindness.\n\n\nInterference is demonstrated in Figure 8; the different point in (a) and (b) is easy to pick out, but it is much harder in (c) to separate shape and color in order to pick out differences. This is because when color and shape are used to show different features, we must consider every combination of color and shape used and individually search for points matching that description – an operation which requires a lot of time and cognitive effort. Not all combinations of color and shape are problematic, however: (d) uses color and shape to show the same variable (dual-encoding), which is useful for individuals who are color blind or may have trouble perceiving shapes. The two preattentive features support each other when used in this way, making it even easier to pick out the one mismatched point than in conditions (a), (b), or (c)."
  },
  {
    "objectID": "content/01-Fundamentals.html#conscious-perception",
    "href": "content/01-Fundamentals.html#conscious-perception",
    "title": "Fundamentals of Graphical Communication",
    "section": "Conscious Perception",
    "text": "Conscious Perception\nWhile it is useful to understand the psychology of perception and the implications of preattentive perception for understanding how much cognitive effort an operation takes, analysts are more concerned about conscious perception that occurs with attention. We care about questions like:\n\nWhich parts of the graph are the most useful for answering a question?\nHow is information from the graph combined with pre-existing knowledge?\nHow does a graph promote understanding of the underlying data?\n\nSeveral different types of models have been proposed to describe this process, but overall, “task models” and “integration models” are most consistent with available empirical evidence.\n\nTask Based Processing\nAn example of task-based processing (Ratwani, Trafton, and Boehm-Davis 2008) is shown in Figure 9.\n\n\n\n\n\nFigure 9: Task-based steps for evaluating the relationship between eruption length and time between eruptions for Old Faithful?\n\n\n\n\nQuestion: What is the relationship between the length of the eruption and the time between eruptions for Old Faithful?\n\nUnderstand the question:\n\nIdentify “length of eruption” and “time between eruptions” as things to search for in the graph.\n\nSearch for identified quantities:\n\nLook for “length of eruption” on the axes and determine that the \\(y\\)-coordinate contains that information.\nLook for “time between eruptions” on the axes and determine that the \\(x\\)-coordinate contains that information.\nVerify that these quantities are what are sought by re-reading the question.\n\nSense-making and Storytelling:\n\nEstablish that as the time between eruptions increases, the length of the eruption increases.\nNote that there seems to be a bimodal distribution of points\n\nAnswer the question:\n\nAs time between eruptions increases, length of the eruption increases.\n\n\nIn practice, the search portion of the task based framework is implicitly connected to the sense-making and storytelling portion, and viewers iterate between the two steps several times before finally proceeding to step 4. The time required for each step may change based on the reader’s familiarity with the task, the chart style, and the background knowledge required to interpret the data. Viewers who are familiar with similar graphics may be able to encode information faster and in larger chunks, answering the question more quickly (Carpenter and Shah 1998).\n\n\nInformation Integration\nCharts designed to promote chunking do so by providing viewers with cues for important features. This can help participants come to conclusions supported by the data and statistical modeling underlying the visual representation, reducing cognitive load.\nFigure 10 shows the same data using three different representations, with an additional representation illustrating the viewer’s mental model. In the first figure, (a), viewers are provided with no additional information and must group points together mentally to make sense of the data; this grouping action takes some cognitive effort, producing something like the second figure, (b). The designer could make this chunking effort less resource-intensive by explicitly coloring by the variable determining groups (if it is known) or by e.g. k-means output, if the clustering is important to the problem at hand.\n\n\n\n\n\n\n\n(a) Data with no additional aesthetics\n\n\n\n\n\n\n\n(b) Perceived chunks in mental representation\n\n\n\n\n\n\n\n\n\n(c) Data, color used to provide chunking information\n\n\n\n\n\n\n\n(d) Data, color + shape used to provide chunking information\n\n\n\n\nFigure 10: When graphical features such as clustering are important, using aesthetics to promote chunking of the information can help readers make sense of the data more efficiently.\n\n\nAnalyzing graphs using task-based models emphasizes the importance of spatial relationships between graphical elements. The gestalt laws of proximity and similarity dictate that items which are close together or physically similar (the same shape or color) are perceived as a group; this spatial perception creates “chunks” of the graph which may be encoded as single objects and thus reduce the mental bandwidth necessary to process the image. Figure 10 shows the advantage of “chunks” in graphs: in (a) there are 120 points that could be grouped into the three clusters shown in (b), but when the clustering is provided using aesthetics, the Gestalt similarity heuristic naturally groups the points for us, without much additional labor.\nAs charts become more complex, it can be difficult to consider all of the perceptual processes which might affect perception and use of a specific visualization. In these cases, cognitive models and frameworks can be extremely useful (Padilla 2018; Padilla et al. 2018). By assessing the interactions between the visualization and the cognitive model, it may be possible to identify potential areas of difficulty that can be addressed in a revised design.\nGraphical conventions are another form of chunking strategy we all use. Figure 11 is one example of what happens when such conventions are violated - the chart inverts the y-axis so that 0 is at the top of the chart and 1000 is at the bottom. When combined with the area chart type, this initially gives the impression of a decline in the population shown on the y-axis, rather than an increase.\n\n\n\nFigure 11: Adherence to graphical conventions facilitates fast and accurate chart comprehension. Violating these conventions can result in misunderstandings and increased comprehension time. Figure from Padilla et al. (2018).\n\n\nOf graphics that present information of similar complexity, graphics that require less effort to understand and search for relevant information are preferable (William S. Cleveland 1985). More complex models of the graphical perception process suggest that data are integrated on a visual level and then on a cognitive level, to form successive clusters of information. Once these clusters are formed, additional information can be integrated by comparing and contrasting different clusters to understand the higher-level meaning in the graph (Ratwani, Trafton, and Boehm-Davis 2008). Graph types which cater to this hierarchical clustering mechanism may be more easily understood by viewers than graphs that do not provide information in a manner easily assimilated by the human brain. Facetted charts may be particularly useful for mapping multidimensional data to provide “chunks” of information in a relevant manner, pre-digested for integration into the viewer’s working conceptual understanding of the dataset. Additionally, color schemes and appropriate labeling of graph features which reduce the amount of work necessary to integrate numerical information from a legend into the visual representation of the graph facilitate graphical inference (Carpenter and Shah 1998)."
  },
  {
    "objectID": "content/01-Fundamentals.html#simple-charts",
    "href": "content/01-Fundamentals.html#simple-charts",
    "title": "Fundamentals of Graphical Communication",
    "section": "Simple Charts",
    "text": "Simple Charts\nA series of experiments by (William S. Cleveland and McGill 1984, 1985, 1987) examined basic perceptual tasks, establishing the relative accuracy of comparisons made using different graphical elements. This ranking is shown in Table 1. Other researchers (C. M. Carswell 1992) have examined a wide range of studies beyond those conducted by Cleveland & McGill and used meta-analysis to collapse this ranking into position/length/angle and area/volume, as the difference in accuracy between categories 1, 2, and 3 is small compared to categories 4 and 5.\n\n\nTable 1: Cleveland & McGill’s ordering of graphical tasks by accuracy. Higher ranking tasks are easier for viewers than low-ranking tasks and should be preferred in graphical design.\n\n\nRank\nTask\n\n\n\n\n1\nPosition (common scale)\n\n\n2\nPosition (non-aligned scale)\n\n\n3\nLength, Direction, Angle, Slope\n\n\n4\nArea\n\n\n5\nVolume, Density, Curvature\n\n\n6\nShading, Color Saturation, Color Hue\n\n\n\n\nIt is important to note that the task asked of participants and the type of chart are both important: when presented with a line graph, viewers are more likely to summarize the graph in terms of the slope of the trend line (even when the x-axis is discrete); when presented with a bar graph, viewers summarize the information using discrete comparisons (M. C. Carswell and Wickens 1987; Shah and Miyake 2005). The task and the graph format interact to influence viewer perceptions, thus, when creating graphics, statisticians should match appropriate graphical formats to meaningful conclusions about the data.\n\nColor, Shape, and Discriminability\nWhile shading, color saturation, and color hue rank poorly in the task hierarchy, this does not mean that they should be used to display information. Rather, it is important to remember that the feature hierarchy in Table 1 was assembled based on numerical estimation accuracy. Estimation accuracy is not the only purpose of a chart - in fact, in general, if estimation accuracy is the only goal, a table is a better representation. While viewers may not be able to correlate a specific color to a specific numerical value, color is capable of providing ordinal information (this point is more saturated than that point) and even order of magnitude level comparisons. As color is a pre-attentive feature processed in parallel, the use of color is a trade-off between numerical precision and cognitive resources (speed and working memory).\nIt is important to consider working memory when constructing graphical scales (particularly when utilizing a discrete scale for categorical data), but it is also important to consider feature selection and discriminability as well. Color is generally believed to be preferrable for representing strata on a scatterplot (William S. Cleveland and McGill 1984), but Lewandowsky and Spence (1989) found that if color is not available or appropriate, shapes, intensity, or discriminable letters may be utilized without a significant decrease in accuracy.\nDiscriminable letters are those which do not share physical features such as closure and symmetry, such as the letters H, Q, and X; confusable letters, such as H, E, and F, are associated with significantly less accurate perception. Demiralp, Bernstein, and Heer (2014) synthesized experimental evaluations of stimuli to create “perceptual kernels” describing the perceived distance between values; multidimensional scaling of the resulting distance matrix produces four distinct groups of shapes which share features (triangles with various degrees of rotation, squares and diamonds, and non-convex shapes such as x, +, and *). This separation suggests that feature integration underlies many of the processing speed effects found in studies examining discrete palettes and scales: palettes which are composed of confusable shapes, letters, or colors will require more processing time (and decrease accuracy) compared with discriminable palettes.\n\n\nOther Considerations\nOther graph features can also influence viewer inferences: multiple studies suggest that our mental schematic for a graph is most consistent with a \\(45^\\circ\\) trend line (William S. Cleveland, McGill, and McGill 1988; Tversky and Schiano 1989). “Banking to \\(45^\\circ\\)” is a commonly-cited recommendation for optimal graphics (it is also quite old, according to Wickham (2013)).\nAxis scale transformations can make it easier for viewers to spot outliers of data conforming to skewed distributions (though this does require some domain-specific knowledge of statistical distributions), and appropriately labeled graphs can reduce the working memory requirements by reducing the number of back-and-forth comparisons required to pass information into working memory (Shah and Miyake 2005).\n\n\nExample - Palmer Penguin Species\nConsider the process of assessing the average of two bars in a bar chart such as Figure 12. Figure 13 (2nd tab) shows the cognitive operations one might require to understand the chart and compute the numerical average between the bars. Figure 14 shows a more conceptual model that includes information storage and access from working memory.\n\nPlotCognitive ProcessesWorking Memory\n\n\n\n\n\n\n\nFigure 12: Palmer Penguin data collected by species. What is the average number of Adelie and Chinstrap Penguins measured? What steps do you go through to calculate this average?\n\n\n\n\n\n\n\n\n\nFigure 13: A cognitive model of processes involved in computing the average height of two bars. From Padilla et al (2018), an adaptation of Pinker (1990)\n\n\n\n\n\n\n\nFigure 14: A more abstract cognitive model that includes working memory From Padilla et al. (2018)\n\n\n\n\n\nIf we change chart types, the mental effort required to determine the average number of Chinstrap and Adelie penguins measured may be much more difficult, or even (nearly) impossible.\n\nBar ChartStacked Bar ChartPie Chart\n\n\n\n\n\nAnnotated illustration of cognitive processes for estimating average bar height.\n\n\n\n\n\n\n\nAnnotated illustration of cognitive processes for estimating average bar height in a stacked bar chart.\n\n\n\n\n\n\n\nAnnotated illustration of cognitive processes for estimating average slice size in a pie chart.\n\n\n\n\n\nAs these examples show, the rough processes involved in estimation are similar for the three versions of the chart, but the amount of working memory involved is not. In the bar chart, it is possible to scaffold the estimate completely off of the axis, without having to subtract or divide explicitly - the midpoint between the two bars can be estimated visually. This is an illustration of the advantage of comparisons between position on an aligned scale relative to not aligned scales."
  },
  {
    "objectID": "content/00-Setup.html",
    "href": "content/00-Setup.html",
    "title": "Setting Up for the Workshop",
    "section": "",
    "text": "This workshop is set up to be largely language-agnostic (at least with regard to programming language). We will provide examples (as necessary) using ggplot2 in R and seaborn or matplotlib in python.\nScreenshots will be provided using RStudio as an IDE for both R and Python, but you are free to use any IDE you wish for this workshop.\n\n\n\n\n\n\nSetting Up R, RStudio, and ggplot2\n\n\n\n\n\nStart here - Instructions for installing R and RStudio for Windows, Mac, and Linux\nOpen R or RStudio. Follow these instructions to install ggplot2 (or the whole tidyverse, if you prefer).\nIf you also plan to use python with RStudio, install the reticulate package after you have installed python.\n\n\n\n\n\n\n\n\n\nSetting Up Python, Seaborn, and Matplotlib\n\n\n\n\n\nStart here - Instructions for installing Python 3 for Windows, Mac, and Linux\nIf you wish, install a development environment such as VSCode or Spyder, or install R and RStudio to use RStudio as an IDE for python.\nInstall Seaborn, which will automatically install matplotlib."
  },
  {
    "objectID": "content/02-eda.html",
    "href": "content/02-eda.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "You have a new dataset and it’s time to dig in. You may have some information about how the data was collected (or not!), but you almost certainly don’t already know what’s in the data - what artifacts, data entry errors, missing data, and outliers may show up. Exploratory Data Analysis, or EDA, is like an exploratory hike through unknown territory. You may have prepared ahead of time - you may have a map, hiking boots, and so on, but you cannot predict what wildlife you will see or what obstacles you might encounter.\nThroughout this section, we’ll use the palmerpenguins data to illustrate different ways to explore data."
  },
  {
    "objectID": "content/02-eda.html#one-dimensional-summaries",
    "href": "content/02-eda.html#one-dimensional-summaries",
    "title": "Exploratory Data Analysis",
    "section": "One-dimensional summaries",
    "text": "One-dimensional summaries\nThere are several basic options for distribution assessment:\n\nBoxplots (numerical data) - visually simple graphical five-number summaries\nHistograms (numerical data) - show full distribution of the data, useful for assessing skewness, etc.\nDensity plot (numerical data) - continuous version of histogram\nBarplots (categorical data) - categorical equivalent of histogram.\n\nIt is often common to initially make these plots for one variable and then to begin to explore conditional distributions by drawing multiple plots for different values of a different variable.\nThe following examples show exploratory plots for the palmerpenguins data.\n\nBoxplots\n\nRPython\n\n\n\nggplot(penguins) + geom_boxplot(aes(y = body_mass_g))\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\nggplot(penguins) + geom_boxplot(aes(x = sex, y = body_mass_g))\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\nggplot(penguins) + geom_boxplot(aes(x = species, y = body_mass_g))\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nsns.boxplot(data = penguins, y = \"body_mass_g\")\nplt.show()\n\n\n\nplt.close()\n\nsns.boxplot(data = penguins, x = \"sex\", y = \"body_mass_g\")\nplt.show()\n\n\n\nplt.close()\n\n\nsns.boxplot(data = penguins, x = \"species\", y = \"body_mass_g\")\nplt.show()\n\n\n\nplt.close()\n\n\n\n\n\n\nHistograms\n\nRPython\n\n\n\nggplot(penguins) + geom_histogram(aes(x = body_mass_g))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\nggplot(penguins) + geom_histogram(aes(x = body_mass_g, fill = sex), alpha = .5, position = \"identity\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\nggplot(penguins) + geom_histogram(aes(x = body_mass_g, fill = species), alpha = .5, position = \"identity\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n(\n  so.Plot(penguins, x = \"body_mass_g\")\n    .add(so.Bars(), so.Hist(bins=30))\n    .show()\n)\n\n\n\nplt.close()\n\n(\n  so.Plot(penguins, x = \"body_mass_g\", color = \"sex\")\n    .add(so.Bars(alpha=.5), so.Hist(bins=30))\n    .show()\n)\n\n\n\nplt.close()\n\n(\n  so.Plot(penguins, x = \"body_mass_g\", color = \"species\")\n    .add(so.Bars(alpha=.5), so.Hist(bins=30))\n    .show()\n)\n\n\n\nplt.close()\n\n\n\n\n\n\nDensity Plots\n\nRPython\n\n\n\nggplot(penguins) + geom_density(aes(x = body_mass_g))\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\nggplot(penguins) + geom_density(aes(x = body_mass_g, fill = sex), alpha = .5)\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\nggplot(penguins) + geom_density(aes(x = body_mass_g, fill = species), alpha = .5)\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\nso.Plot(penguins, x = \"body_mass_g\").add(so.Area(), so.KDE()).show()\n\n\n\nplt.close()\n\n(\n  so.Plot(penguins, x = \"body_mass_g\", color = \"sex\")\n    .add(so.Area(), so.KDE(common_norm=False))\n    .show()\n)\n\n\n\nplt.close()\n\n(\n  so.Plot(penguins, x = \"body_mass_g\", color = \"species\")\n    .add(so.Area(), so.KDE(common_norm=False))\n    .show()\n)\n\n\n\nplt.close()\n\n\n\n\n\n\nBar Plots\n\nRPython\n\n\n\nggplot(penguins) + geom_bar(aes(x = species))\n\n\n\nggplot(penguins) + geom_bar(aes(x = species, fill = sex), position = \"dodge\")\n\n\n\nggplot(penguins) + geom_bar(aes(x = species, fill = island))\n\n\n\n\n\n\n\n(\n  so.Plot(penguins, x = \"species\")\n    .add(so.Bar(), so.Count())\n    .show()\n)\n\n\n\nplt.close()\n\n(\n  so.Plot(penguins, x = \"species\", color = \"sex\")\n    .add(so.Bar(), so.Count(), so.Dodge())\n    .show()\n)\n\n\n\nplt.close()\n\n(\n  so.Plot(penguins, x = \"species\", color = \"island\")\n    .add(so.Bar(), so.Count(), so.Stack())\n    .show()\n)\n\n\n\nplt.close()"
  },
  {
    "objectID": "content/02-eda.html#two-dimensional-summaries",
    "href": "content/02-eda.html#two-dimensional-summaries",
    "title": "Exploratory Data Analysis",
    "section": "Two-dimensional summaries",
    "text": "Two-dimensional summaries\nSome of the one-dimensional summaries discussed above are easily modified into two dimensional summaries by coloring by or faceting by another variable. Other two-dimensional summaries require different types of plots: for instance, scatter plots, which show two numerical variables and can be modified to use color, shape, and facets to include even more information.\n\nScatterplots\n\nRPython\n\n\n\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point() \n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = species)) +\n  geom_point() \n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = species)) +\n  geom_point() +\n  facet_wrap(~year)\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n(\n  so.Plot(penguins, x = \"bill_length_mm\", y = \"bill_depth_mm\")\n    .add(so.Dot())\n    .show()\n)\n\n\n\nplt.close()\n\n(\n  so.Plot(penguins, x = \"bill_length_mm\", y = \"bill_depth_mm\", color = \"species\")\n    .add(so.Dot())\n    .show()\n)\n\n\n\nplt.close()\n\n(\n  so.Plot(penguins, x = \"bill_length_mm\", y = \"bill_depth_mm\", color = \"species\")\n    .facet(\"year\")\n    .add(so.Dot())\n    .show()\n)\n\n\n\nplt.close()\n\n\n\n\n\n\nScatter plot Matrices\nSometimes, we want to examine conditional relationships between many different variables at a time - for instance, to identify highly correlated variables when considering fitting a linear model. Scatter plot matrices allow us to examine relationships between many sets of two variables at once.\n\nRPython\n\n\n\nlibrary(GGally)\nggpairs(penguins, columns = 3:6, aes(color = species))\n\n\n\n\n\n\n\npenguin_measures = penguins.drop(\"year\", axis = 1)\nsns.pairplot(penguin_measures, hue = \"species\")\n\n\n\nplt.close()"
  },
  {
    "objectID": "content/contents.html",
    "href": "content/contents.html",
    "title": "Workshop Materials",
    "section": "",
    "text": "Setting Up for the Workshop\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nFundamentals of Graphical Communication\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nExploratory Data Analysis\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nAccessibility: More than just Alt-text\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nThe Rules, and when to break them…\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nWorkshop: Improving Our Graphics\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "slides/01-Fundamentals.html#the-eye",
    "href": "slides/01-Fundamentals.html#the-eye",
    "title": "Fundamentals of Graphical Communication",
    "section": "The Eye",
    "text": "The Eye\n\nFigure 1: The human eye, with closeup of receptor cells in the retina. Image Source License Authors: OpenStax Copyright Holders: Rice University Publishers: OpenStax OpenStax Biology\nImportant parts:\n\nrods and cones - transform light waves into neural signals\n\nrods: light intensity (light/dark)\ncones: light wavelength (color)\n\nblindspot - region where the optic nerve exits the back of the eye. We don’t notice this because our brain compensates.\nfovea - region of mostly cones, highest acuity, but you can’t notice e.g. faint stars because you have so many fewer rods in this area. Focusing on a point involves looking at it with your fovea, generally speaking."
  },
  {
    "objectID": "slides/01-Fundamentals.html#color-vision",
    "href": "slides/01-Fundamentals.html#color-vision",
    "title": "Fundamentals of Graphical Communication",
    "section": "Color Vision",
    "text": "Color Vision\n\nFigure 2: Absorption spectra of rods and short (blue), medium (green), and long (red) wave cones. Image modified from Pancrat’s Wikimedia Commons work. License.\nNotice that we have better color differentiation in the yellow-green portion of the visual spectrum, compared to red/blue. Sun’s light is primarily yellow, most plants are green - this is evolutionary optimization at work. It also is one reason why rainbow color schemes are hard to use - we differentiate different parts of the spectrum with higher resolution, so rainbows don’t map to perceptual space well."
  },
  {
    "objectID": "slides/01-Fundamentals.html#the-brain-hardware",
    "href": "slides/01-Fundamentals.html#the-brain-hardware",
    "title": "Fundamentals of Graphical Communication",
    "section": "The Brain (Hardware)",
    "text": "The Brain (Hardware)\n\nSignals from the retina are integrated - multiple rods combined together\n\nResults in optical illusions - Hermann grid, Mach bands\n\nFeature detectors - parts of the brain that recognize lines at specific angles, in spatial arrays\nSpecialized modules for e.g. face detection\nLots of additional processing – “software”"
  },
  {
    "objectID": "slides/01-Fundamentals.html#selective-attention",
    "href": "slides/01-Fundamentals.html#selective-attention",
    "title": "Fundamentals of Graphical Communication",
    "section": "Selective Attention",
    "text": "Selective Attention"
  },
  {
    "objectID": "slides/01-Fundamentals.html#selective-attention-1",
    "href": "slides/01-Fundamentals.html#selective-attention-1",
    "title": "Fundamentals of Graphical Communication",
    "section": "Selective Attention",
    "text": "Selective Attention\nPreattentive features such as color, shape, position, are integrated and applied to single objects through focused attention\n\nEyes On Me Focus Stickerfrom Eyes On Me Stickers"
  },
  {
    "objectID": "demo/penguins-ggplot2.html",
    "href": "demo/penguins-ggplot2.html",
    "title": "Ggplot2 Penguins",
    "section": "",
    "text": "Let’s create a plot of the penguins data using ggplot2.\n\ndata &lt;- read.csv(\"../data/penguins.csv\")\nlibrary(ggplot2)\nggplot(data, aes(x = bill_length_mm, y = bill_depth_mm, color = species)) + geom_point()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "demo/penguins-obs.html",
    "href": "demo/penguins-obs.html",
    "title": "Observable Penguins",
    "section": "",
    "text": "data=FileAttachment(\"../data/penguins.csv\").csv({typed: true})\n\n\n\n\n\n\nWe can create a tabular representation of the full dataset:\n\nInputs.table(data)\n\n\n\n\n\n\nLet’s create a simple scatterplot of bill_length_mm vs bill_depth_mm:\n\nPlot.plot({\n  marks: [\n    Plot.dot(data, {x: \"bill_length_mm\", y: \"bill_depth_mm\", fill: \"species\"})\n  ],\n  color: {legend: true}\n})"
  }
]
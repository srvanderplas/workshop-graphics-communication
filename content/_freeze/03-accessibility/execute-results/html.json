{
  "hash": "99752030ac8c1e134e20ff884869e4b9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Accessibility: More than just Alt-text\"\ninclude-before-body:\n  text: |\n    <!-- olli -->\n    <script src=\"https://cdn.jsdelivr.net/npm/olli@2\"></script>\n    <script src=\"https://cdn.jsdelivr.net/npm/olli-adapters@2\"></script>\n---\n\n\nGraphics are one of the primary tools for science or data communication, and are powerful because they make use of our visual system in a way that off-loads much of the work of processing the data, freeing cognitive resources up to consider the content rather than the representation.\nUnfortunately, not everyone can leverage their visual systems in this way, due to differences in the visual system or in information processing systems within the brain.\nThere are a wide range of issues which may impact how effectively people can use graphics, including colorblindness, poor visual acuity, blindness, dyslexia, dyscalculia, and even differences in data literacy and numerical literacy. \nIn some sense, it can be useful to go through the introspection process when looking at a graph, and consider what perceptual and cognitive resources are required to complete each step when looking at a chart. \nUltimately, as scientists and people working with data, we need to work to make our data representations accessible. \nThe specifics of which populations we focus on, and how we adapt existing representations (or create new ones) are based on the target audience(s), and will differ across different disciplines. \nFor instance, if you are designing graphics to be used in Air Traffic Controller trainings, you likely do not need to accommodate Blind and Low Vision (BLV) individuals or even consider color-blindness. \nHowever, if you are creating graphics to be consumed by a general web audience, it is important to consider a range of visual impairments and accommodations. \n\n\nThis is an active area of research in data science, information visualization, and design. \nIt is always useful to see what new solutions have come out recently, because there are new developments in this area on a regular basis.\n\n\n# Adapting Existing Graphics\n\nOne of the simplest ways to increase accessibility of graphics is to ensure that they meet basic guidelines for discoverability and distinguishability.\nDiscoverability ensures that screen-reader users know that the graphic exists; this requires designing the entire web page with these users in mind, but is critical to ensuring equal access to information for blind and low-vision users.\nCreating graphics which adhere to contrast, color selection, font size, and other distinguishability guidelines helps low-vision people, those with sensory processing issues, and people with colorblindness use existing data representations effectively.\n\n## Discoverability\n\n### Alt Text\n\nWhen retrofitting an existing page for accessibility, it may not be possible to make charts and graphics fully accessible to individuals who are blind or using screen-readers. In these cases, it is important to write good alt-text for each graph that is intended to convey information (it is fine to skip alt-text for purely decorative images). \n\nGood alt-text is:\n\n- concise\n- accurate\n- relevant\n- context-dependent - the same image may require different alt-text depending on the broader context of the web page.\n\nGraphs are some of the hardest images to fully describe in alt-text, in part because providing the same information that the image provides may require thousands of words, full data tables, or other accommodations. \nThe alt text field in HTML does not allow for paragraphs, line breaks, and other structural elements; as a result, it is often better to include a short description in the alt-text field and a longer description (or table) as part of the web page [source](https://sc.edu/about/offices_and_divisions/digital-accessibility/toolbox/best_practices/alternative_text/charts-diagrams/index.php). \nLinking the alt-text and the longer description together may facilitate keyboard navigation between the two, making the navigation process less cognitively intensive for screen reader users.\n\nThe `BrailleR` R package integrates with many common R plotting functions (including base graphics and `ggplot2`) and can generate some functional alt-text automatically. \n\n::: {.callout collapse=\"true\"}\n### BrailleR alt-text demo\n\n\n::: {.cell hash='03-accessibility_cache/html/unnamed-chunk-1_70a03479e0ff15b38a2882feb783fe9d'}\n\n```{.r .cell-code}\nlibrary(BrailleR)\n\ndata <- read.csv(\"../data/penguins.csv\")\nlibrary(ggplot2)\nscatterplot <- ggplot(data, \n                      aes(x = bill_length_mm, \n                          y = bill_depth_mm, \n                          color = species)) + \n  geom_point()\n\nscatterplot\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](03-accessibility_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nThis is an untitled chart with no subtitle or caption.\nIt has x-axis 'bill_length_mm' with labels 40, 50 and 60.\nIt has y-axis 'bill_depth_mm' with labels 15.0, 17.5 and 20.0.\nThere is a legend indicating colour is used to show species, with 3 levels:\nAdelie shown as strong reddish orange colour, \nChinstrap shown as vivid yellowish green colour and \nGentoo shown as brilliant blue colour.\nThe chart is a set of 342 big solid circle points of which about 97% can be seen.\n```\n\n\n:::\n:::\n\n\n:::\n\n\n### Page Structure\n\nThe entire web-page structure is important to consider when designing to include screen-reader users. \nSighted users may be able to take in the web page structure visually and determine which elements to focus on; screen reader users have to take in the page structure audibly, and in sequence. Some users describe it as \"viewing a web page through a straw\".\n\nIt is important to provide a contextual overview first, and then provide specific data and details in a structured hierarchy.\nWe can update the \"information seeking mantra\" of overview, zoom and filter, details on demand to \"gist\", \"supporting methods\" (contextual information), and \"details\" (actual data content). \nThe structure of the page needs to support understanding when navigated hierarchically. \nInformation embedded in visual design themes (font sizes, colors, nesting, space) need to be explicitly accessible via aria- fields to be accessible to screen reader users.\n\n::: {callout-tip}\n#### Navigating with a Screen Reader\n\n\n{{< video https://youtu.be/SRT0vCCRjRA >}}\n\n\n\n:::\n\n## Discriminability\n\nIn addition to the information below, which includes some web references, there is a lovely series of Observable posts on accessibility, contrast, and color choice for data visualization. [Check it out!](https://observablehq.com/@frankelavsky/chartability-contrast-series)\n\n### Color selection\n\nWe'll first approach color selection with color impairment (aka \"colorblindness\", though most color impaired people can see some colors) in mind, though many of the considerations here factor into contrast considerations later.\nThere are several approaches to accommodating color impairment:\n\n1. Avoid red and green combinations. This helps but is not sufficient, particularly for those who have trouble with red and blue, rather than green.\n\n2. Use palettes designed to be \"colorblind-friendly\", such as [David Nichol's](https://davidmathlogic.com/colorblind/#%23D81B60-%231E88E5-%23FFC107-%23004D40), [Okabe-Ito's](https://jfly.uni-koeln.de/color/), [Paul Tol's](https://thenode.biologists.com/data-visualization-with-flying-colors/research/). Colorbrewer's colorblind friendly palettes are less useful than these options.\n\n3. Design your graphic so that it is functional in greyscale. This will make it safe for all types of color impairment.\n\n4. Dual-encode colors with other attributes, such as shape or linetype. \n\nIt can be difficult to fully accommodate those with color impairment, particularly when working with graphics that use many different hues. Keep in mind that even people with full color vision cannot keep more than $7 \\pm 2$ items in working memory - so using many different colors is problematic for everyone, not just for those with impaired color vision. \n\n### Contrast\n\nIt can be hard to see content that does not have much contrast against the background. \nPeople with low vision rely on contrast even more than the rest of the population; in addition, individuals with color impairment tend to rely on contrast cues to determine whether ambiguous colors are, in fact, different.\n\nW3C (World Wide Web Consortium) creates Web Content Accessibility Guidelines (WCAG) to provide a standard of accessible online content. [These guidelines](https://www.w3.org/TR/WCAG21/) have recommendations for creating alt-text, how to ensure accessibility of different types of media, and standards for how to make content distinguishable.\n\nWCAG guidelines are provided on a scale from A (basic accessibility) to AAA (most accessible). \n\n::: {.callout collapse=\"false\"}\n#### (A) guidelines for distinguishability:\n\n- [Color is not used as the only visual means of conveying information, indicating an action, prompting a response, or distinguishing a visual element.](https://www.w3.org/TR/WCAG21/#use-of-color)\n\n- [Audio Control: If any audio on a Web page plays automatically for more than 3 seconds, either a mechanism is available to pause or stop the audio, or a mechanism is available to control audio volume independently from the overall system volume level.](https://www.w3.org/TR/WCAG21/#audio-control)\n:::\n\n\n::: {.callout collapse=\"false\"}\n#### (AA) guidelines for distinguishability:\n\n- [Contrast: The visual presentation of text and images of text has a contrast ratio of at least 4.5:1, except for the following:](https://www.w3.org/TR/WCAG21/#contrast-minimum) \n    - Large Text: Large-scale text and images of large-scale text have a contrast ratio of at least 3:1; \n    - Incidental: Text or images of text that are part of an inactive user interface component, that are pure decoration, that are not visible to anyone, or that are part of a picture that contains significant other visual content, have no contrast requirement. \n    - Logo: Text that is part of a logo or brand name has no contrast requirement.\n\n- [Resize Text: Except for captions and images of text, text can be resized without assistive technology up to 200 percent without loss of content or functionality.](https://www.w3.org/TR/WCAG21/#resize-text)\n\n- [Images of Text: If the technologies being used can achieve the visual presentation, text is used to convey information rather than images of text except for the following:](https://www.w3.org/TR/WCAG21/#images-of-text)\n    - Customizable: The image of text can be visually customized to the user's requirements; \n    - Essential: A particular presentation of text is essential to the information being conveyed. Logotypes (text that is part of a logo or brand name) are considered essential.\n\n- [Reflow: Content can be presented without loss of information or functionality, and without requiring scrolling in two dimensions for:](https://www.w3.org/TR/WCAG21/#reflow)\n    - Vertical scrolling content at a width equivalent to 320 CSS pixels;\n    - Horizontal scrolling content at a height equivalent to 256 CSS pixels.\n    - Except for parts of the content which require two-dimensional layout for usage or meaning.\n    \n- [Non-text Contrast: The visual presentation of the following have a contrast ratio of at least 3:1 against adjacent color(s):](https://www.w3.org/TR/WCAG21/#non-text-contrast)\n    - User Interface Components: Visual information required to identify user interface components and states, except for inactive components or where the appearance of the component is determined by the user agent and not modified by the author;\n    - Graphical Objects: Parts of graphics required to understand the content, except when a particular presentation of graphics is essential to the information being conveyed. \n    \n- [Text Spacing: In content implemented using markup languages that support the following text style properties, no loss of content or functionality occurs by setting all of the following and by changing no other style property:](https://www.w3.org/TR/WCAG21/#text-spacing)\n    - Line height (line spacing) to at least 1.5 times the font size;\n    - Spacing following paragraphs to at least 2 times the font size;\n    - Letter spacing (tracking) to at least 0.12 times the font size;\n    - Word spacing to at least 0.16 times the font size.\n    - Exception: Human languages and scripts that do not make use of one or more of these text style properties in written text can conform using only the properties that exist for that combination of language and script.\n    \n- [Content on Hover or Focus: Where receiving and then removing pointer hover or keyboard focus triggers additional content to become visible and then hidden, the following are true:](https://www.w3.org/TR/WCAG21/#content-on-hover-or-focus)\n    - Dismissible: A mechanism is available to dismiss the additional content without moving pointer hover or keyboard focus, unless the additional content communicates an input error or does not obscure or replace other content;\n    - Hoverable: If pointer hover can trigger the additional content, then the pointer can be moved over the additional content without the additional content disappearing;\n    - Persistent: The additional content remains visible until the hover or focus trigger is removed, the user dismisses it, or its information is no longer valid.\n    - Exception: The visual presentation of the additional content is controlled by the user agent and is not modified by the author.\n:::\n   \n\n::: {.callout collapse=\"false\"} \n#### (AAA) guidelines for distinguishability:\n\n- [Enhanced Contrast: The visual presentation of text and images of text has a contrast ratio of at least 7:1, except for the following:](https://www.w3.org/TR/WCAG21/#contrast-enhanced)\n    - Large Text: Large-scale text and images of large-scale text have a contrast ratio of at least 4.5:1;\n    - Incidental: Text or images of text that are part of an inactive user interface component, that are pure decoration, that are not visible to anyone, or that are part of a picture that contains significant other visual content, have no contrast requirement.\n    - Logotypes: Text that is part of a logo or brand name has no contrast requirement.\n\n- [Low or No Background Audio: For prerecorded audio-only content that (1) contains primarily speech in the foreground, (2) is not an audio CAPTCHA or audio logo, and (3) is not vocalization intended to be primarily musical expression such as singing or rapping, at least one of the following is true:](https://www.w3.org/TR/WCAG21/#low-or-no-background-audio)\n    - No Background: The audio does not contain background sounds.\n    - Turn Off: The background sounds can be turned off.\n    - 20 dB: The background sounds are at least 20 decibels lower than the foreground speech content, with the exception of occasional sounds that last for only one or two seconds.\n    \n- [Visual Presentation: For the visual presentation of blocks of text, a mechanism is available to achieve the following:](https://www.w3.org/TR/WCAG21/#visual-presentation)\n    - Foreground and background colors can be selected by the user.\n    - Width is no more than 80 characters or glyphs (40 if CJK).\n    - Text is not justified (aligned to both the left and the right margins).\n    - Line spacing (leading) is at least space-and-a-half within paragraphs, and paragraph spacing is at least 1.5 times larger than the line spacing.\n    - Text can be resized without assistive technology up to 200 percent in a way that does not require the user to scroll horizontally to read a line of text on a full-screen window.\n\n- [Images of Text: Images of text are only used for pure decoration or where a particular presentation of text is essential to the information being conveyed.](https://www.w3.org/TR/WCAG21/#images-of-text-no-exception)\n:::\n\nYou can use [https://www.accessibilitychecker.org/](https://www.accessibilitychecker.org/) to check the compliance of your website. \n\n[Chartability](https://chartability.fizz.studio/) is a set of heuristics for ensuring accessibility of data visualizations (and the pages that contain them). It's created by BLV designers and is designed to help you locate accessibility barriers in data visualizations. They maintain an audit [workbook](https://chartability.github.io/POUR-CAF/) that has tests that help identify design failures.\n\n\n# Creating More Accessible Graphics\n\nIn general, charts created as images, which are the default in many systems such as `ggplot`, `matplotlib`, and `SAS`, require alt-text, inclusion of data tables, and other modifications that still do not produce full accessibility. \nBy contrast, `d3`, `Observable.js`, `Highcharts.js`, and other `svg`-based web graphics allow for some navigation within the chart by screen reader users. \nHowever, these tools still require some extra planning to design charts that are accessible and well-formatted for screen-reader users.\n\n\nThe [OLLi](https://mitvis.github.io/olli/) project works within Observable and Vega/VegaLite visualizations to create a navigable, hierarchical tree for keyboard navigation and descriptions. \n\n\nLet's consider three different pages produced using quarto to showcase the penguins data. \nExplore the graphics yourself first, then play the video below to see how my screen reader handled each one.\nNote the level of detail available to the user.\n\n::: panel-tabset\n\n### ggplot2\n\n\n::: {.cell hash='03-accessibility_cache/html/fig-ggplot2-penguins_e8f488af947623a1f73fdadfc5e352d6'}\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nThis is an untitled chart with no subtitle or caption.\nIt has x-axis 'bill_length_mm' with labels 40, 50 and 60.\nIt has y-axis 'bill_depth_mm' with labels 15.0, 17.5 and 20.0.\nThere is a legend indicating colour is used to show species, with 3 levels:\nAdelie shown as strong reddish orange colour, \nChinstrap shown as vivid yellowish green colour and \nGentoo shown as brilliant blue colour.\nThe chart is a set of 342 big solid circle points of which about 97% can be seen.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![A plot of the palmer penguins data, showing bill length in mm on the x axis and bill depth in mm on the y axis. Points are colored by species. For each species, there is a positive relationship between bill depth and bill length, with each species occupying a different region of the space. Adelie penguins have shorter, deeper bills. Chinstrap penguins have longer, deeper bills. Gentoo penguins have longer, shallower bills.](03-accessibility_files/figure-html/fig-ggplot2-penguins-1.png){#fig-ggplot2-penguins width=672}\n:::\n:::\n\n\n\n### Observable.js\n\n\n\n```{ojs}\n//| echo: false\n//| fig-cap: A plot of the palmer penguins data, showing bill length in mm on the x axis and bill depth in mm on the y axis. Points are colored by species. For each species, there is a positive relationship between bill depth and bill length, with each species occupying a different region of the space. Adelie penguins have shorter, deeper bills. Chinstrap penguins have longer, deeper bills. Gentoo penguins have longer, shallower bills.\n//| label: fig-observable-penguins\n\ndata=FileAttachment(\"../data/penguins.csv\").csv({typed: true})\nPlot.plot({\n  marks: [\n    Plot.dot(data, {x: \"bill_length_mm\", y: \"bill_depth_mm\", fill: \"species\"})\n  ],\n  color: {legend: true}\n})\n```\n\n\n\n### Observable.js + Olli\n\n\n\n```{ojs}\n//| echo: false\npenguinChart = ({\n  marks: [\n    Plot.dot(data, {x: \"bill_length_mm\", y: \"bill_depth_mm\", fill: \"species\"})\n  ],\n  color: {legend: true}\n})\n\n```\n\n```{ojs}\n//| label: fig-olli-penguin\n//| echo: false\n//| fig-cap: A plot of the palmer penguins data, showing bill length in mm on the x axis and bill depth in mm on the y axis. Points are colored by species. For each species, there is a positive relationship between bill depth and bill length, with each species occupying a different region of the space. Adelie penguins have shorter, deeper bills. Chinstrap penguins have longer, deeper bills. Gentoo penguins have longer, shallower bills.\n// Create plot with specification\nPlot.plot(penguinChart);\n```\n\n\n\n\n:::\n\n\n{{< video https://youtu.be/EsMsBqfB_Vs >}}\n\n\n\nOlli is a fantastic project, but support for Observable is limited; there is better support for Vega charts, but even then, not all chart types are supported.\n\n## Sonification\n\nThere are other methods of communicating data without relying primarily on visual methods and adapting those representations to remove reliance on vision.\n@zongUmweltAccessibleStructured2024 developed Umwelt, which allows for editing of multimodal data representations, providing some support for sonification and non-visual data communication. \nThere are also methods for adapting existing visualizations to produce sonified equivalents, using tools like [`Strauss`](https://github.com/james-trayford/strauss) or even using `miditools` [@russoSonificationConvertData2024]. @hermannSonificationHandbook2011 is an excellent reference for the process of data sonification. \n\n::: {.callout collapse=\"true\"}\n\n### Sonifying Data With Python\n\nHere's an example of how to create a data sonification using the penguins data, adapted from @russoSonificationConvertData2024. \n\n\n::: {.cell hash='03-accessibility_cache/html/unnamed-chunk-7_3c0d77638a2de738cd1e697831755138'}\n\n```{.python .cell-code}\n# Code adapted from https://hub.ovh2.mybinder.org/user/systemsounds-so-ation-tutorials-vr3cdobo/doc/tree/data2midi-part1.ipynb\n\nimport pandas as pd\n\npenguins = pd.read_csv(\"../data/penguins.csv\").dropna()\n\n# Define a general mapping function\ndef map_value(value, min_value, max_value, min_result, max_result):\n  '''maps value (or array of values) from one range to another'''\n  \n  result = min_result + (value - min_value)/(max_value - min_value)*(max_result - min_result)\n  return result\n\n# Set desired duration: 15 seconds/beats\npenguins.bill_length_mm.describe # get info on penguin bill lengths\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<bound method NDFrame.describe of 0      39.1\n1      39.5\n2      40.3\n4      36.7\n5      39.3\n       ... \n339    55.8\n340    43.5\n341    49.6\n342    50.8\n343    50.2\nName: bill_length_mm, Length: 333, dtype: float64>\n```\n\n\n:::\n\n```{.python .cell-code}\npenguins = penguins.sort_values(by=['species', 'bill_length_mm'], ascending = True) # sort data by bill length\n\nduration_beats = 15\nbpm = 60\nduration_sec = duration_beats*60/bpm\n\n# Scale x axis\npenguins[\"t_data\"] = map_value(penguins.bill_length_mm, min(penguins.bill_length_mm), max(penguins.bill_length_mm), 0, duration_beats)\n\n# Scale y axis\npenguins[\"y_data\"] = map_value(penguins.bill_depth_mm, min(penguins.bill_depth_mm), max(penguins.bill_depth_mm), 0, 1)\n\n# May want to transform data a bit - example uses sqrt\n# y_scale = 0.5\n# \n# penguins[\"y_data\"] = penguins.y_data**y_scale\n\nimport matplotlib.pyplot as plt\n\nplt.scatter(penguins.t_data, penguins.y_data)\nplt.xlabel('time (bill length, mm, normalized)')\nplt.ylabel('bill depth, mm, normalized')\nplt.show()\n```\n\n::: {.cell-output-display}\n![](03-accessibility_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n```{.python .cell-code}\nplt.clf()\n\n# Scale penguin species\npenguins[\"track\"] = penguins.species.replace({\"Adelie\":0,\"Gentoo\":1,\"Chinstrap\":2})\nfor track_i in range(3):\n  dat = penguins.query(\"track==@track_i\")\n  plt.scatter(dat.t_data, dat.y_data)\n\nplt.xlabel('time (bill length, mm, normalized)')\nplt.ylabel('bill depth, mm, normalized')\nplt.show()\n```\n\n::: {.cell-output-display}\n![](03-accessibility_files/figure-html/unnamed-chunk-7-2.png){width=672}\n:::\n:::\n\n\n\n\n\nNow that we've transformed the data, we can map data values to tones. \nMIDI tones and velocities are integers, so we must transform and then round our values to match the requirements of the medium we're using.\n\n\n::: {.cell hash='03-accessibility_cache/html/unnamed-chunk-9_92481a89a91cfdea97a0389c4431da5f'}\n\n```{.python .cell-code}\n# In this case, I'm happy with just 2 octaves of chromatic notes, from C3 to C5. \n# These correspond to MIDI notes 48:72\n\npenguins[\"midi_y\"] = round(map_value(penguins.y_data, 0, 1, 48, 72))\npenguins[\"midi_y\"] = penguins.midi_y.convert_dtypes()\nfor track_i in range(3):\n  dat = penguins.query(\"track==@track_i\")\n  plt.scatter(dat.t_data, dat.midi_y)\nplt.xlabel('time (bill length, mm, normalized)')\nplt.ylabel('MIDI note number')\nplt.show()\n```\n\n::: {.cell-output-display}\n![](03-accessibility_files/figure-html/unnamed-chunk-9-7.png){width=672}\n:::\n\n```{.python .cell-code}\n\n# Map data to note velocity - velocity is a combination of volume and intensity\n# We dual-encode pitch and velocity\n# \n# vel_min, vel_max = 35, 127\n# penguins[\"midi_vel\"] = round(map_value(penguins.y_data, 0, 1, vel_max, vel_min))\n# penguins[\"midi_vel\"] = penguins.midi_vel.convert_dtypes()\n```\n:::\n\n\n\n\nFinally, we create the midi file. \nWe add 3 tracks, one for each species - this would allow us to change the \"program\" (instrument) to correspond to each species group.\nFor the moment, I've commented the program changes out because the addition of instruments makes the end result sound like elementary school band warm-up time (complete chaos). \n\n\n::: {.cell hash='03-accessibility_cache/html/unnamed-chunk-11_d35ca9ffbd291a22ba34155edac3236c'}\n\n```{.python .cell-code}\n\nfrom midiutil.MidiFile import MIDIFile #import library to make midi file, https://midiutil.readthedocs.io/en/1.2.1/\n    \n#create midi file object, add tempo\nmy_midi_file = MIDIFile(3, deinterleave=False) #three tracks, one for each species \nmy_midi_file.addTempo(track=0, time=0, tempo=bpm) \nmy_midi_file.addTempo(track=1, time=0, tempo=bpm) \nmy_midi_file.addTempo(track=2, time=0, tempo=bpm) \n\n# \n# # .addProgramChange(track, channel, time, program)\n# my_midi_file.addProgramChange(0, 0, 0, 71) # first set of penguins as clarinet\n# my_midi_file.addProgramChange(1, 0, 0, 75) # second set of penguins as pan flute\n# my_midi_file.addProgramChange(2, 0, 0, 59) # third set of penguins as muted trumpets\n\n#add midi notes\nfor i in penguins.index:\n    my_midi_file.addNote(track=penguins.track[i], channel=0, pitch=penguins.midi_y[i], time=penguins.t_data[i], duration=0.25, volume=35)\n\n#create and save the midi file itself\nfilename = 'penguins_sonification.mid'\nwith open(filename, \"wb\") as f:\n    my_midi_file.writeFile(f) \n\n# Listen\n# import pygame #import library for playing midi files, https://pypi.org/project/pygame/\n# pygame.init()\n# pygame.mixer.music.load(filename)\n# pygame.mixer.music.play()\n```\n:::\n\n\n\n\n:::\n\n::: {.column-margin}\n\n\n::: {.cell hash='03-accessibility_cache/html/unnamed-chunk-13_da416accc7443c87ea394c3632d578c3'}\n::: {.cell-output-display}\n\n```{=html}\n<audio controls=\"\">\n<source src=\"../fig/penguins_sonification.mp3\" type=\"audio/mpeg\"/>\n</audio>\n```\n\n:::\n:::\n\n\nResults of sonification of @fig-ggplot2-penguins using python and MIDI audio encoding. \n\n:::\n\n## Physicalization\n\nThere are a number of ways to create accessible tactile charts using embossing machines, capsule paper [@braunerCreatingTactileGraphic2023], or 3D printers. \nTactile graphics have higher performance than tactile tables or electronic tables accessed via screen reader [@watanabeEffectivenessTactileScatter2018], in addition, tactile bar charts presented either alone or with auditory information have higher performance than audio-only presentation [@goncuUsabilityAccessibleBar2010].\n\nR packages like `rayshader` can be used to convert `ggplot2` plots into 3D-printable STL files [@morganwallCreateMapsVisualize2024]. \nThis produces a STL file that has some tactile information, without requiring too much specialized software; it could be made more accessible by using a Braille font.\nOne downside is that the height of the plot object is mapped to color/fill, and does not accommodate categorical mappings. \n\n::: {.callout collapse=\"true\"}\n\n### Rayshader demo\n\n\n::: {.cell hash='03-accessibility_cache/html/unnamed-chunk-14_308e16590faf923ac9ded7686badb0ab'}\n\n```{.r .cell-code}\ndata <- read.csv(\"../data/penguins.csv\")\nlibrary(ggplot2)\nplot <- ggplot(data) + \n  stat_density_2d(aes(x = bill_length_mm, y = bill_depth_mm, \n                      fill = after_stat(!!str2lang(\"density\"))),\n                  contour = F, geom = \"raster\") +\n  scale_x_continuous(expand=c(0,0)) +\n  scale_y_continuous(expand=c(0,0))\n\nlibrary(rayshader)\nplot_gg(plot, width = 6, windowsize=c(1400,866),\n       zoom = 0.55, theta = -10, phi = 25, scale=300, \n       emboss_text = .05)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density2d()`).\nRemoved 2 rows containing non-finite outside the scale range\n(`stat_density2d()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nThis is an untitled chart with no subtitle or caption.\nIt has x-axis 'bill_length_mm' with labels 40 and 50.\nIt has y-axis 'bill_depth_mm' with labels 14, 16, 18 and 20.\nThere is a legend indicating fill is used to show density, ranging from 5.50919365996529e-09 represented by fill dark purplish blue to 0.0160915333509369 shown as fill brilliant blue.\nThe chart is a raster graph that VI cannot process.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density2d()`).\n```\n\n\n:::\n\n```{.r .cell-code}\nrender_snapshot()\n```\n\n::: {.cell-output-display}\n![](03-accessibility_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n\n```{.r .cell-code}\nsave_3dprint(\"../fig/penguins-density-plot.stl\", maxwidth = 125, unit = \"mm\", rotate = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nDimensions of model are: 125.0 mm x 62.5 mm x 25.0 mm\n```\n\n\n:::\n:::\n\n:::\n\n# References\n\n<!-- - Basic overview of types of different accessibility concerns -->\n<!--     - Blind/Low Vision -->\n<!--     - Data & numerical literacy -->\n<!--     - Processing disorders - dyslexia, dyscalculia, ADHD -->\n\n<!-- - Approaches to navigating disability -->\n<!--     - Screen reader  - try out navigating with a screen reader around this site. I've spent very little time on things like alt-text mainly for the purposes of demonstration. -->\n\n<!-- - Visual Solutions -->\n<!--     - Low vision support -->\n\n<!--     - Level 1: Basic compliance/communication -->\n<!--         - Alt text -->\n<!--         - Data Tables -->\n<!--     - Level 2: Engagement -->\n<!--         - Keyboard navigation of visualization content -->\n<!--         - Sonification [@sakhardandeComparingUserPerformance2019;@acarturkDevelopingVerbalAssistance2014] -->\n<!--         - Physicalization [@watanabeEffectivenessTactileScatter2018;@goncuUsabilityAccessibleBar2010] -->\n\n\n\n\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}